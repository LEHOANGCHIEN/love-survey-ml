{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063b5870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE A — Data Understanding & Audit\n",
      "Reading file path: D:\\UIT\\Semester 5\\Vocational Skills\\love_survey_responses.xlsx\n",
      "Exists: True\n",
      "\n",
      "Rows: 208, Columns: 52\n",
      "\n",
      "=== 5 rows preview ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Giới tính của bạn là?</th>\n",
       "      <th>Độ tuổi của bạn là?</th>\n",
       "      <th>Xu hướng tính dục của bạn là?</th>\n",
       "      <th>Tôn giáo của bạn là?</th>\n",
       "      <th>Tình trạng việc làm hiện tại của bạn?</th>\n",
       "      <th>Bạn đánh giá sức khoẻ thể chất của mình như thế nào?</th>\n",
       "      <th>Bạn cảm thấy ngoại hình của mình như thế nào?</th>\n",
       "      <th>Trình độ học vấn hiện tại của bạn?</th>\n",
       "      <th>Bạn hiện có đang trong một mối quan hệ tình cảm không?</th>\n",
       "      <th>...</th>\n",
       "      <th>Những lỗi lầm nào có thể tha thứ trong tình yêu?</th>\n",
       "      <th>Bạn có tin vào tình yêu sét đánh không?</th>\n",
       "      <th>Bạn nghĩ tình yêu có cần gắn với tình dục không?</th>\n",
       "      <th>Bạn nghĩ ở tuổi nào nên kết hôn là hợp lý?</th>\n",
       "      <th>Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn như thế nào?</th>\n",
       "      <th>Score</th>\n",
       "      <th>Email Address</th>\n",
       "      <th>Column 48</th>\n",
       "      <th>Column 48.1</th>\n",
       "      <th>Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-11 17:33:36.626</td>\n",
       "      <td>Nam</td>\n",
       "      <td>18 - 22</td>\n",
       "      <td>Dị tính</td>\n",
       "      <td>Không có</td>\n",
       "      <td>Chỉ học</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Đại học</td>\n",
       "      <td>Không</td>\n",
       "      <td>...</td>\n",
       "      <td>Trễ giờ, Ăn mặc bất lịch sự</td>\n",
       "      <td>Không</td>\n",
       "      <td>Tuỳ mối quan hệ</td>\n",
       "      <td>Dưới 25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-11 17:57:18.692</td>\n",
       "      <td>Nam</td>\n",
       "      <td>18 - 22</td>\n",
       "      <td>Đồng tính</td>\n",
       "      <td>Không có</td>\n",
       "      <td>Chỉ học</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Đại học</td>\n",
       "      <td>Không</td>\n",
       "      <td>...</td>\n",
       "      <td>Trễ giờ, Ăn mặc bất lịch sự, Ăn nói thiếu chuẩ...</td>\n",
       "      <td>Không</td>\n",
       "      <td>Không</td>\n",
       "      <td>28 – 30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-11 18:01:03.520</td>\n",
       "      <td>Nam</td>\n",
       "      <td>18 - 22</td>\n",
       "      <td>Dị tính</td>\n",
       "      <td>Không có</td>\n",
       "      <td>Chỉ học</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Đại học</td>\n",
       "      <td>Không</td>\n",
       "      <td>...</td>\n",
       "      <td>Trễ giờ, Ăn mặc bất lịch sự</td>\n",
       "      <td>Có</td>\n",
       "      <td>Tuỳ mối quan hệ</td>\n",
       "      <td>Dưới 25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-11 18:04:04.443</td>\n",
       "      <td>Nam</td>\n",
       "      <td>18 - 22</td>\n",
       "      <td>Dị tính</td>\n",
       "      <td>Không có</td>\n",
       "      <td>Chỉ học</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Đại học</td>\n",
       "      <td>Không</td>\n",
       "      <td>...</td>\n",
       "      <td>Trễ giờ, Ăn mặc bất lịch sự, Ăn nói thiếu chuẩ...</td>\n",
       "      <td>Không</td>\n",
       "      <td>Tuỳ mối quan hệ</td>\n",
       "      <td>28 – 30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-11 18:05:53.782</td>\n",
       "      <td>Nam</td>\n",
       "      <td>18 - 22</td>\n",
       "      <td>Khác</td>\n",
       "      <td>Không có</td>\n",
       "      <td>Chỉ học</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Đại học</td>\n",
       "      <td>Không</td>\n",
       "      <td>...</td>\n",
       "      <td>Không tôn trọng gia đình người yêu</td>\n",
       "      <td>Không</td>\n",
       "      <td>Tuỳ mối quan hệ</td>\n",
       "      <td>28 – 30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp Giới tính của bạn là? Độ tuổi của bạn là?  \\\n",
       "0 2025-10-11 17:33:36.626                   Nam             18 - 22   \n",
       "1 2025-10-11 17:57:18.692                   Nam             18 - 22   \n",
       "2 2025-10-11 18:01:03.520                   Nam             18 - 22   \n",
       "3 2025-10-11 18:04:04.443                   Nam             18 - 22   \n",
       "4 2025-10-11 18:05:53.782                   Nam             18 - 22   \n",
       "\n",
       "  Xu hướng tính dục của bạn là? Tôn giáo của bạn là?  \\\n",
       "0                       Dị tính             Không có   \n",
       "1                     Đồng tính             Không có   \n",
       "2                       Dị tính             Không có   \n",
       "3                       Dị tính             Không có   \n",
       "4                          Khác             Không có   \n",
       "\n",
       "  Tình trạng việc làm hiện tại của bạn?  \\\n",
       "0                               Chỉ học   \n",
       "1                               Chỉ học   \n",
       "2                               Chỉ học   \n",
       "3                               Chỉ học   \n",
       "4                               Chỉ học   \n",
       "\n",
       "   Bạn đánh giá sức khoẻ thể chất của mình như thế nào?  \\\n",
       "0                                                  3      \n",
       "1                                                  5      \n",
       "2                                                  3      \n",
       "3                                                  5      \n",
       "4                                                  5      \n",
       "\n",
       "   Bạn cảm thấy ngoại hình của mình như thế nào?  \\\n",
       "0                                              3   \n",
       "1                                              2   \n",
       "2                                              2   \n",
       "3                                              2   \n",
       "4                                              3   \n",
       "\n",
       "  Trình độ học vấn hiện tại của bạn?  \\\n",
       "0                            Đại học   \n",
       "1                            Đại học   \n",
       "2                            Đại học   \n",
       "3                            Đại học   \n",
       "4                            Đại học   \n",
       "\n",
       "  Bạn hiện có đang trong một mối quan hệ tình cảm không?  ...  \\\n",
       "0                                              Không      ...   \n",
       "1                                              Không      ...   \n",
       "2                                              Không      ...   \n",
       "3                                              Không      ...   \n",
       "4                                              Không      ...   \n",
       "\n",
       "    Những lỗi lầm nào có thể tha thứ trong tình yêu?  \\\n",
       "0                        Trễ giờ, Ăn mặc bất lịch sự   \n",
       "1  Trễ giờ, Ăn mặc bất lịch sự, Ăn nói thiếu chuẩ...   \n",
       "2                        Trễ giờ, Ăn mặc bất lịch sự   \n",
       "3  Trễ giờ, Ăn mặc bất lịch sự, Ăn nói thiếu chuẩ...   \n",
       "4                 Không tôn trọng gia đình người yêu   \n",
       "\n",
       "  Bạn có tin vào tình yêu sét đánh không?  \\\n",
       "0                                   Không   \n",
       "1                                   Không   \n",
       "2                                      Có   \n",
       "3                                   Không   \n",
       "4                                   Không   \n",
       "\n",
       "  Bạn nghĩ tình yêu có cần gắn với tình dục không?  \\\n",
       "0                                  Tuỳ mối quan hệ   \n",
       "1                                            Không   \n",
       "2                                  Tuỳ mối quan hệ   \n",
       "3                                  Tuỳ mối quan hệ   \n",
       "4                                  Tuỳ mối quan hệ   \n",
       "\n",
       "  Bạn nghĩ ở tuổi nào nên kết hôn là hợp lý?  \\\n",
       "0                                    Dưới 25   \n",
       "1                                    28 – 30   \n",
       "2                                    Dưới 25   \n",
       "3                                    28 – 30   \n",
       "4                                    28 – 30   \n",
       "\n",
       "   Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn như thế nào?  \\\n",
       "0                                                NaN                         \n",
       "1                                                NaN                         \n",
       "2                                                NaN                         \n",
       "3                                                NaN                         \n",
       "4                                                NaN                         \n",
       "\n",
       "  Score  Email Address Column 48 Column 48.1  \\\n",
       "0     0            NaN       NaN         NaN   \n",
       "1     0            NaN       NaN         NaN   \n",
       "2     0            NaN       NaN         NaN   \n",
       "3     0            NaN       NaN         NaN   \n",
       "4     0            NaN       NaN         NaN   \n",
       "\n",
       "   Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?  \n",
       "0                                                NaN                  \n",
       "1                                                NaN                  \n",
       "2                                                NaN                  \n",
       "3                                                NaN                  \n",
       "4                                                NaN                  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns (index : name) ===\n",
      "001. Timestamp\n",
      "002. Giới tính của bạn là?\n",
      "003. Độ tuổi của bạn là?\n",
      "004. Xu hướng tính dục của bạn là?\n",
      "005. Tôn giáo của bạn là?\n",
      "006. Tình trạng việc làm hiện tại của bạn?\n",
      "007. Bạn đánh giá sức khoẻ thể chất của mình như thế nào?\n",
      "008. Bạn cảm thấy ngoại hình của mình như thế nào?\n",
      "009. Trình độ học vấn hiện tại của bạn?\n",
      "010. Bạn hiện có đang trong một mối quan hệ tình cảm không?\n",
      "011. Nếu chưa thì có những lý do nào khiến bạn chưa có người yêu?\n",
      "012. Bạn có mong muốn tìm được người yêu trong tương lai gần không?\n",
      "013. Bạn mong muốn mối quan hệ trong tương lai sẽ kéo dài bao lâu?\n",
      "014. Tiêu chí quan trọng nhất với bạn khi chọn người yêu?\n",
      "015. Bạn có cảm thấy áp lực từ gia đình hoặc xã hội về việc phải có người yêu không?\n",
      "016. Bạn đã từng/ thử tiếp cận tình yêu qua cách nào?\n",
      "017. BẠN CÓ TIN VÀO TÌNH YÊU SÉT ĐÁNH KHÔNG?\n",
      "018. Bạn đã từng trải qua một mối tình chính thức chưa?\n",
      "019. Nếu có thì bạn đã chia tay vì những lý do gì?\n",
      "020. BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?\n",
      "021. Column 20\n",
      "022. Mối quan hệ của bạn đã kéo dài bao lâu?\n",
      "023. Bạn và người yêu hiện tại quen nhau qua phương thức nào?\n",
      "024. Trong tình yêu, bạn cảm thấy yếu tố nào là quan trọng nhất?\n",
      "025. Mức độ hài lòng của bạn về mối quan hệ hiện tại?\n",
      "026. BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  \n",
      "027. Bạn và người yêu có thường xuyên xung đột không?\n",
      "028. Bạn và người yêu có cùng quan điểm về tình dục không?\n",
      "029. Nếu bạn cảm thấy người yêu của mình quá gần gũi người khác, mức độ ghen tị của bạn ở đâu?\n",
      "030. Nếu bạn đang ở trong một mối quan hệ nhưng gặp người khác phù hợp hơn, bạn có ý định chấm dứt mối quan hệ hiện tại để theo đuổi người đó không?\n",
      "031. Bạn và người yêu có chia sẻ chung mục tiêu tương lai (học tập, nghề nghiệp) không?\n",
      "032. Bạn có mong muốn kết hôn với người yêu hiện tại không?\n",
      "033. Bạn nghĩ ai nên trả tiền khi hẹn hò?\n",
      "034. Có nên chia sẻ thường xuyên chuyện hẹn hò cho gia đình không?\n",
      "035. Nên ưu tiên yếu tố gì khi chọn địa điểm hẹn hò cùng người yêu?\n",
      "036. Có nên tìm kiếm người yêu khi đang xây dựng sự nghiệp?\n",
      "037. Tình yêu ảnh hưởng đến kết quả học tập/ sự nghiệp ở mức độ như thế nào?\n",
      "038. Theo bạn, đâu là lý do khiến nhiều sinh viên chia tay nhất?\n",
      "039. Nên chi tiêu bao nhiêu hàng tháng cho việc hẹn hò?\n",
      "040. Có nên yêu xa hay không?\n",
      "041. Yếu tố nào gây mất điểm nhất khi hẹn hò?\n",
      "042. Có nên nghe lời khuyên từ bạn bè trong việc hẹn hò không?\n",
      "043. Những lỗi lầm nào có thể tha thứ trong tình yêu?\n",
      "044. Bạn có tin vào tình yêu sét đánh không?\n",
      "045. Bạn nghĩ tình yêu có cần gắn với tình dục không?\n",
      "046. Bạn nghĩ ở tuổi nào nên kết hôn là hợp lý?\n",
      "047. Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn như thế nào?\n",
      "048. Score\n",
      "049. Email Address\n",
      "050. Column 48\n",
      "051. Column 48.1\n",
      "052. Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?\n",
      "\n",
      "Columns 100% empty (cần drop): ['BẠN CÓ TIN VÀO TÌNH YÊU SÉT ĐÁNH KHÔNG?', 'BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?', 'Column 20', 'BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  ', 'Column 48', 'Column 48.1', 'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?']\n",
      "Columns with <=1 unique value: ['BẠN CÓ TIN VÀO TÌNH YÊU SÉT ĐÁNH KHÔNG?', 'BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?', 'Column 20', 'BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  ', 'Score', 'Column 48', 'Column 48.1', 'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?']\n",
      "Exact duplicate columns (content-wise): {'BẠN CÓ TIN VÀO TÌNH YÊU SÉT ĐÁNH KHÔNG?': ['BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?', 'Column 20', 'BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  ', 'Column 48', 'Column 48.1', 'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?']}\n",
      "Columns likely Timestamp/Date-like: ['Timestamp']\n",
      "Columns likely Email-like: ['Email Address']\n",
      "\n",
      "Numeric-like columns (inferred): ['Bạn đánh giá sức khoẻ thể chất của mình như thế nào?', 'Bạn cảm thấy ngoại hình của mình như thế nào?', 'Bạn có cảm thấy áp lực từ gia đình hoặc xã hội về việc phải có người yêu không?', 'BẠN CÓ TIN VÀO TÌNH YÊU SÉT ĐÁNH KHÔNG?', 'BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?', 'Column 20', 'Mức độ hài lòng của bạn về mối quan hệ hiện tại?', 'BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  ', 'Nếu bạn cảm thấy người yêu của mình quá gần gũi người khác, mức độ ghen tị của bạn ở đâu?', 'Tình yêu ảnh hưởng đến kết quả học tập/ sự nghiệp ở mức độ như thế nào?', 'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn như thế nào?', 'Score', 'Column 48', 'Column 48.1', 'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?']\n",
      "Categorical/Text-like columns (inferred): ['Timestamp', 'Giới tính của bạn là?', 'Độ tuổi của bạn là?', 'Xu hướng tính dục của bạn là?', 'Tôn giáo của bạn là?', 'Tình trạng việc làm hiện tại của bạn?', 'Trình độ học vấn hiện tại của bạn?', 'Bạn hiện có đang trong một mối quan hệ tình cảm không?', 'Nếu chưa thì có những lý do nào khiến bạn chưa có người yêu?', 'Bạn có mong muốn tìm được người yêu trong tương lai gần không?'] ...\n",
      "\n",
      "Columns likely free-text (high cardinality):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_33956\\2633851631.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if dup_mask[i]:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>n_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Email Address</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           column  n_unique\n",
       "48  Email Address       165"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audit saved to: data\\audit_columns.csv  (open this file to review suggested actions)\n",
      "\n",
      "PHASE A complete: audit saved. Please paste the full output (and/or content of data/audit_columns.csv) here so I can review and then send PHASE B cell.\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell: PHASE A — Data Understanding & Audit (1 cell = toàn Phase A) =====\n",
    "# Mục tiêu: kiểm tra file, audit cột, lưu audit_columns.csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ---------- CẤU HÌNH ĐƯỜNG DẪN (KHÔNG THAY ĐỔI) ----------\n",
    "DATA_PATH = Path(r\"D:\\UIT\\Semester 5\\Vocational Skills\\love_survey_responses.xlsx\")\n",
    "PROJECT_DATA_DIR = Path(\"data\")  # nơi lưu audit csv của project (không phải file gốc)\n",
    "\n",
    "# ---------- HÀM HỖ TRỢ ----------\n",
    "def top_values_str(s, n=3):\n",
    "    try:\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        items = [f\"{idx} ({cnt})\" for idx, cnt in zip(vc.index.astype(str).tolist()[:n], vc.values[:n])]\n",
    "        return \" | \".join(items)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def first_nonnull_samples(s, n=3):\n",
    "    try:\n",
    "        samples = s.dropna().astype(str).unique().tolist()[:n]\n",
    "        return \" | \".join(samples)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# ---------- CHẠY AUDIT ----------\n",
    "print(\"PHASE A — Data Understanding & Audit\")\n",
    "print(\"Reading file path:\", DATA_PATH)\n",
    "print(\"Exists:\", DATA_PATH.exists())\n",
    "print()\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"File not found. Vui lòng kiểm tra lại đường dẫn chính xác: {DATA_PATH}\")\n",
    "\n",
    "# đọc file (openpyxl)\n",
    "df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "\n",
    "n_rows, n_cols = df.shape\n",
    "print(f\"Rows: {n_rows}, Columns: {n_cols}\")\n",
    "print()\n",
    "\n",
    "print(\"=== 5 rows preview ===\")\n",
    "display(df.head())\n",
    "\n",
    "# danh sách cột\n",
    "print(\"\\n=== Columns (index : name) ===\")\n",
    "for i, c in enumerate(df.columns, 1):\n",
    "    print(f\"{i:03d}. {c}\")\n",
    "\n",
    "# cột 100% empty\n",
    "empty_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "print(\"\\nColumns 100% empty (cần drop):\", empty_cols)\n",
    "\n",
    "# cột chỉ có 1 unique value\n",
    "single_value_cols = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]\n",
    "print(\"Columns with <=1 unique value:\", single_value_cols)\n",
    "\n",
    "# exact duplicate columns (by content)\n",
    "# transpose and find duplicated rows -> duplicated columns\n",
    "dup_mask = df.T.duplicated(keep='first')\n",
    "dup_columns = df.columns[dup_mask].tolist()\n",
    "dup_groups = {}\n",
    "if dup_columns:\n",
    "    # group duplicates by their first occurrence\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if dup_mask[i]:\n",
    "            # find the first column equal to this one\n",
    "            for j in range(i):\n",
    "                if df.iloc[:, j].equals(df.iloc[:, i]):\n",
    "                    dup_groups.setdefault(df.columns[j], []).append(df.columns[i])\n",
    "                    break\n",
    "\n",
    "print(\"Exact duplicate columns (content-wise):\", dup_groups if dup_groups else [])\n",
    "\n",
    "# detect likely timestamp/email columns by name\n",
    "likely_timestamp = [c for c in df.columns if \"time\" in c.lower() or \"timestamp\" in c.lower() or \"date\" in c.lower()]\n",
    "likely_email = [c for c in df.columns if \"email\" in c.lower()]\n",
    "print(\"Columns likely Timestamp/Date-like:\", likely_timestamp)\n",
    "print(\"Columns likely Email-like:\", likely_email)\n",
    "\n",
    "# dtype summary, missing rate, unique counts, top values\n",
    "audit_rows = []\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    n_missing = int(s.isna().sum())\n",
    "    missing_rate = float(n_missing) / n_rows\n",
    "    n_unique = int(s.nunique(dropna=False))\n",
    "    dtype = str(s.dtype)\n",
    "    top_values = top_values_str(s, n=3)\n",
    "    samples = first_nonnull_samples(s, n=3)\n",
    "    # mark free-text-like: object dtype and high cardinality (>50% rows unique)\n",
    "    free_text_like = False\n",
    "    try:\n",
    "        if dtype in (\"object\", \"string\") and n_unique > 0.5 * n_rows:\n",
    "            free_text_like = True\n",
    "    except Exception:\n",
    "        free_text_like = False\n",
    "    audit_rows.append({\n",
    "        \"column\": c,\n",
    "        \"dtype\": dtype,\n",
    "        \"n_missing\": n_missing,\n",
    "        \"missing_rate\": round(missing_rate, 4),\n",
    "        \"n_unique\": n_unique,\n",
    "        \"top_values\": top_values,\n",
    "        \"sample_values\": samples,\n",
    "        \"is_all_empty\": c in empty_cols,\n",
    "        \"is_single_value\": c in single_value_cols,\n",
    "        \"is_exact_dup_of\": next((k for k, v in dup_groups.items() if c in v), \"\"),\n",
    "        \"likely_timestamp\": c in likely_timestamp,\n",
    "        \"likely_email\": c in likely_email,\n",
    "        \"free_text_like\": free_text_like\n",
    "    })\n",
    "\n",
    "audit_df = pd.DataFrame(audit_rows)\n",
    "\n",
    "# thêm một vài tóm tắt\n",
    "numeric_cols = audit_df[audit_df[\"dtype\"].str.contains(\"int|float|number\", case=False, na=False)][\"column\"].tolist()\n",
    "categorical_hint = audit_df[~audit_df[\"column\"].isin(numeric_cols)][\"column\"].tolist()\n",
    "\n",
    "print(\"\\nNumeric-like columns (inferred):\", numeric_cols)\n",
    "print(\"Categorical/Text-like columns (inferred):\", categorical_hint[:10], \"...\" if len(categorical_hint) > 10 else \"\")\n",
    "\n",
    "# high-cardinality text columns\n",
    "high_card_text = audit_df[(audit_df[\"free_text_like\"])][[\"column\", \"n_unique\"]]\n",
    "if not high_card_text.empty:\n",
    "    print(\"\\nColumns likely free-text (high cardinality):\")\n",
    "    display(high_card_text)\n",
    "\n",
    "# Save audit csv\n",
    "PROJECT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "audit_path = PROJECT_DATA_DIR / \"audit_columns.csv\"\n",
    "audit_df.to_csv(audit_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nAudit saved to: {audit_path}  (open this file to review suggested actions)\")\n",
    "\n",
    "# Save a short JSON summary for quick programmatic use\n",
    "summary = {\n",
    "    \"rows\": n_rows,\n",
    "    \"cols\": n_cols,\n",
    "    \"n_empty_columns\": len(empty_cols),\n",
    "    \"n_single_value_columns\": len(single_value_cols),\n",
    "    \"n_exact_duplicate_groups\": len(dup_groups),\n",
    "    \"likely_timestamp_columns\": likely_timestamp,\n",
    "    \"likely_email_columns\": likely_email\n",
    "}\n",
    "with open(PROJECT_DATA_DIR / \"audit_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nPHASE A complete: audit saved. Please paste the full output (and/or content of data/audit_columns.csv) here so I can review and then send PHASE B cell.\")\n",
    "# ============================================================================== \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcef1562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (208, 52) rows x columns\n",
      "After dropping unusable columns: (208, 43)\n",
      "Columns after renaming:\n",
      "['giới_tính_của_bạn_là', 'độ_tuổi_của_bạn_là', 'xu_hướng_tính_dục_của_bạn_là', 'tôn_giáo_của_bạn_là', 'tình_trạng_việc_làm_hiện_tại_của_bạn', 'bạn_đánh_giá_sức_khoẻ_thể_chất_của_mình_như_thế_nào', 'bạn_cảm_thấy_ngoại_hình_của_mình_như_thế_nào', 'trình_độ_học_vấn_hiện_tại_của_bạn', 'bạn_hiện_có_đang_trong_một_mối_quan_hệ_tình_cảm_không', 'nếu_chưa_thì_có_những_lý_do_nào_khiến_bạn_chưa_có_người_yêu', 'bạn_có_mong_muốn_tìm_được_người_yêu_trong_tương_lai_gần_không', 'bạn_mong_muốn_mối_quan_hệ_trong_tương_lai_sẽ_kéo_dài_bao_lâu', 'tiêu_chí_quan_trọng_nhất_với_bạn_khi_chọn_người_yêu', 'bạn_có_cảm_thấy_áp_lực_từ_gia_đình_hoặc_xã_hội_về_việc_phải_có_người_yêu_không', 'bạn_đã_từng_thử_tiếp_cận_tình_yêu_qua_cách_nào', 'label_b_has_had_official_relationship', 'nếu_có_thì_bạn_đã_chia_tay_vì_những_lý_do_gì', 'mối_quan_hệ_của_bạn_đã_kéo_dài_bao_lâu', 'bạn_và_người_yêu_hiện_tại_quen_nhau_qua_phương_thức_nào', 'trong_tình_yêu_bạn_cảm_thấy_yếu_tố_nào_là_quan_trọng_nhất', 'mức_độ_hài_lòng_của_bạn_về_mối_quan_hệ_hiện_tại', 'bạn_và_người_yêu_có_thường_xuyên_xung_đột_không', 'bạn_và_người_yêu_có_cùng_quan_điểm_về_tình_dục_không', 'nếu_bạn_cảm_thấy_người_yêu_của_mình_quá_gần_gũi_người_khác_mức_độ_ghen_tị_của_bạn_ở_đâu', 'nếu_bạn_đang_ở_trong_một_mối_quan_hệ_nhưng_gặp_người_khác_phù_hợp_hơn_bạn_có_ý_định_chấm_dứt_mối_quan_hệ_hiện_tại_để_theo_đuổi_người_đó_không', 'bạn_và_người_yêu_có_chia_sẻ_chung_mục_tiêu_tương_lai_học_tập_nghề_nghiệp_không', 'bạn_có_mong_muốn_kết_hôn_với_người_yêu_hiện_tại_không', 'bạn_nghĩ_ai_nên_trả_tiền_khi_hẹn_hò', 'có_nên_chia_sẻ_thường_xuyên_chuyện_hẹn_hò_cho_gia_đình_không', 'nên_ưu_tiên_yếu_tố_gì_khi_chọn_địa_điểm_hẹn_hò_cùng_người_yêu', 'có_nên_tìm_kiếm_người_yêu_khi_đang_xây_dựng_sự_nghiệp', 'tình_yêu_ảnh_hưởng_đến_kết_quả_học_tập_sự_nghiệp_ở_mức_độ_như_thế_nào', 'theo_bạn_đâu_là_lý_do_khiến_nhiều_sinh_viên_chia_tay_nhất', 'nên_chi_tiêu_bao_nhiêu_hàng_tháng_cho_việc_hẹn_hò', 'có_nên_yêu_xa_hay_không', 'yếu_tố_nào_gây_mất_điểm_nhất_khi_hẹn_hò', 'có_nên_nghe_lời_khuyên_từ_bạn_bè_trong_việc_hẹn_hò_không', 'những_lỗi_lầm_nào_có_thể_tha_thứ_trong_tình_yêu', 'label_a_love_at_first_sight', 'bạn_nghĩ_tình_yêu_có_cần_gắn_với_tình_dục_không', 'bạn_nghĩ_ở_tuổi_nào_nên_kết_hôn_là_hợp_lý', 'bạn_cảm_thấy_gia_đình_có_ảnh_hưởng_đến_mối_quan_hệ_của_bạn_như_thế_nào', 'score']\n",
      "\n",
      "PHASE B COMPLETE — cleaned file saved to: data\\clean_data_v1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_33956\\3023055198.py:59: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(normalize_yes_no)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_33956\\3023055198.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(normalize_likert)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell: PHASE B — Data Cleaning =====\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load file nguồn\n",
    "# -----------------------------\n",
    "DATA_PATH = Path(r\"D:\\UIT\\Semester 5\\Vocational Skills\\love_survey_responses.xlsx\")\n",
    "OUTPUT_PATH = Path(\"data/clean_data_v1.csv\")\n",
    "\n",
    "df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "print(\"Loaded:\", df.shape, \"rows x columns\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Drop các cột rác theo Phase A\n",
    "# -----------------------------\n",
    "cols_empty = [\n",
    "    'BẠN CÓ TIN VÀO TÌNH YÊU SÉT ĐÁNH KHÔNG?',\n",
    "    'BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?',\n",
    "    'Column 20',\n",
    "    'BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  ',\n",
    "    'Column 48',\n",
    "    'Column 48.1',\n",
    "    'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?',\n",
    "]\n",
    "\n",
    "cols_dup = [\n",
    "    'BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?',\n",
    "    'Column 20',\n",
    "    'BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  ',\n",
    "    'Column 48',\n",
    "    'Column 48.1',\n",
    "    'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?',\n",
    "]\n",
    "\n",
    "cols_email = ['Email Address']\n",
    "cols_timestamp = ['Timestamp']\n",
    "\n",
    "cols_to_drop = list(set(cols_empty + cols_dup + cols_email + cols_timestamp))\n",
    "cols_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
    "\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(\"After dropping unusable columns:\", df.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Chuẩn hóa Có/Không → 1/0\n",
    "# -----------------------------\n",
    "def normalize_yes_no(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    x = str(x).strip().lower()\n",
    "    if x in [\"có\", \"yes\", \"y\", \"1\", \"co\", \"true\"]:\n",
    "        return 1\n",
    "    if x in [\"không\", \"ko\", \"khong\", \"no\", \"0\", \"false\"]:\n",
    "        return 0\n",
    "    return x  # giá trị khác giữ nguyên\n",
    "\n",
    "df = df.applymap(normalize_yes_no)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Chuẩn hóa các câu Likert (5 mức)\n",
    "# -----------------------------\n",
    "likert_map = {\n",
    "    \"rất không đồng ý\": 1,\n",
    "    \"không đồng ý\": 2,\n",
    "    \"phân vân\": 3,\n",
    "    \"trung lập\": 3,\n",
    "    \"đồng ý\": 4,\n",
    "    \"rất đồng ý\": 5,\n",
    "}\n",
    "\n",
    "def normalize_likert(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    x_low = str(x).strip().lower()\n",
    "    return likert_map.get(x_low, x)  # không map được thì giữ nguyên\n",
    "\n",
    "df = df.applymap(normalize_likert)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Rename tất cả cột về dạng slug an toàn\n",
    "#    + Tạo mapping đặc biệt cho Label A/B\n",
    "# -----------------------------\n",
    "def slugify(name):\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    name = re.sub(\"_+\", \"_\", name)\n",
    "    return name.lower()\n",
    "\n",
    "rename_map = {c: slugify(c) for c in df.columns}\n",
    "\n",
    "# Override cho 2 labels\n",
    "for original, new in [\n",
    "    (\"Bạn có tin vào tình yêu sét đánh không?\", \"label_a_love_at_first_sight\"),\n",
    "    (\"Bạn đã từng trải qua một mối tình chính thức chưa?\", \"label_b_has_had_official_relationship\"),\n",
    "]:\n",
    "    if original in rename_map:\n",
    "        rename_map[original] = new\n",
    "\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "print(\"Columns after renaming:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Lưu output\n",
    "# -----------------------------\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nPHASE B COMPLETE — cleaned file saved to:\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ed790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: D:\\UIT\\Semester 5\\Vocational Skills\\love_survey_responses.xlsx\n",
      "Original shape: (208, 52)\n",
      "Detected to_drop: empty=7, dup=6, ts=1, email=1, explicit_junk=4\n",
      "Example to_drop (up to 20): ['BẠN CÓ COI TÌNH YÊU HIỆN TẠI LÀ BƯỚC ĐẾN HÔN NHÂN KHÔNG  ', 'BẠN CÓ TIN VÀO TÌNH YÊU SÉT ĐÁNH KHÔNG?', 'BẠN NGHĨ Ở ĐỘ TUỔI NÀO THÌ NÊN KẾT HÔN?', 'Bạn cảm thấy gia đình có ảnh hưởng đến mối quan hệ của bạn không?', 'Column 20', 'Column 48', 'Column 48.1', 'Email Address', 'Score', 'Timestamp']\n",
      "Saved col rename map to: data\\col_rename_map.csv\n",
      "After drop shape: (208, 42)\n",
      "After rename sample cols: ['giới_tính_của_bạn_là', 'độ_tuổi_của_bạn_là', 'xu_hướng_tính_dục_của_bạn_là', 'tôn_giáo_của_bạn_là', 'tình_trạng_việc_làm_hiện_tại_của_bạn', 'bạn_đánh_giá_sức_khoẻ_thể_chất_của_mình_như_thế_nào', 'bạn_cảm_thấy_ngoại_hình_của_mình_như_thế_nào', 'trình_độ_học_vấn_hiện_tại_của_bạn', 'bạn_hiện_có_đang_trong_một_mối_quan_hệ_tình_cảm_không', 'nếu_chưa_thì_có_những_lý_do_nào_khiến_bạn_chưa_có_người_yêu', 'bạn_có_mong_muốn_tìm_được_người_yêu_trong_tương_lai_gần_không', 'bạn_mong_muốn_mối_quan_hệ_trong_tương_lai_sẽ_kéo_dài_bao_lâu']\n",
      "\n",
      "PHASE B (fixed) COMPLETE.\n",
      "Cleaned primary dataset saved: data\\clean_data_v1.csv  (rows=208, cols=42)\n",
      "Per-label datasets saved: labelA -> data\\clean_for_labelA.csv (rows=208), labelB -> data\\clean_for_labelB.csv (rows=147)\n",
      "Rename map saved: data\\col_rename_map.csv\n",
      "Cleaning changes saved: data\\cleaning_changes.json\n",
      "\n",
      "Dropped columns count: 10\n",
      "Numeric columns imputed (count by col): {'bạn_có_cảm_thấy_áp_lực_từ_gia_đình_hoặc_xã_hội_về_việc_phải_có_người_yêu_không': 61, 'mức_độ_hài_lòng_của_bạn_về_mối_quan_hệ_hiện_tại': 147, 'nếu_bạn_cảm_thấy_người_yêu_của_mình_quá_gần_gũi_người_khác_mức_độ_ghen_tị_của_bạn_ở_đâu': 147, 'bạn_cảm_thấy_gia_đình_có_ảnh_hưởng_đến_mối_quan_hệ_của_bạn_như_thế_nào': 147}\n",
      "Categorical columns imputed (count by col): {'nếu_chưa_thì_có_những_lý_do_nào_khiến_bạn_chưa_có_người_yêu': 61, 'bạn_có_mong_muốn_tìm_được_người_yêu_trong_tương_lai_gần_không': 61, 'bạn_mong_muốn_mối_quan_hệ_trong_tương_lai_sẽ_kéo_dài_bao_lâu': 61, 'tiêu_chí_quan_trọng_nhất_với_bạn_khi_chọn_người_yêu': 61, 'bạn_đã_từng_thử_tiếp_cận_tình_yêu_qua_cách_nào': 61, 'nếu_có_thì_bạn_đã_chia_tay_vì_những_lý_do_gì': 61, 'mối_quan_hệ_của_bạn_đã_kéo_dài_bao_lâu': 147, 'bạn_và_người_yêu_hiện_tại_quen_nhau_qua_phương_thức_nào': 147, 'trong_tình_yêu_bạn_cảm_thấy_yếu_tố_nào_là_quan_trọng_nhất': 147, 'bạn_và_người_yêu_có_thường_xuyên_xung_đột_không': 147, 'bạn_và_người_yêu_có_cùng_quan_điểm_về_tình_dục_không': 149, 'nếu_bạn_đang_ở_trong_một_mối_quan_hệ_nhưng_gặp_người_khác_phù_hợp_hơn_bạn_có_ý_định_chấm_dứt_mối_quan_hệ_hiện_tại_để_theo_đuổi_người_đó_không': 147, 'bạn_và_người_yêu_có_chia_sẻ_chung_mục_tiêu_tương_lai_học_tập_nghề_nghiệp_không': 147, 'bạn_có_mong_muốn_kết_hôn_với_người_yêu_hiện_tại_không': 147}\n",
      "Rows dropped for labelA: 0 Rows dropped for labelB: 61\n",
      "\n",
      "If you want labels imputed instead of dropping rows, tell me and I will change strategy.\n"
     ]
    }
   ],
   "source": [
    "# ===== PHASE B (FIXED) — Full Data Cleaning (1 cell) =====\n",
    "# - Reads original Excel at D:\\UIT\\Semester 5\\Vocational Skills\\love_survey_responses.xlsx\n",
    "# - Outputs:\n",
    "#   data/col_rename_map.csv\n",
    "#   data/clean_data_v1.csv\n",
    "#   data/clean_for_labelA.csv\n",
    "#   data/clean_for_labelB.csv\n",
    "#   data/cleaning_changes.json\n",
    "# Notes: DOES NOT impute label columns; impute only features.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re, json\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATA_PATH = Path(r\"D:\\UIT\\Semester 5\\Vocational Skills\\love_survey_responses.xlsx\")\n",
    "OUT_DIR = Path(\"data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "COL_RENAME_PATH = OUT_DIR / \"col_rename_map.csv\"\n",
    "CLEAN_V1_PATH = OUT_DIR / \"clean_data_v1.csv\"\n",
    "CLEAN_A_PATH = OUT_DIR / \"clean_for_labelA.csv\"\n",
    "CLEAN_B_PATH = OUT_DIR / \"clean_for_labelB.csv\"\n",
    "CHANGES_PATH = OUT_DIR / \"cleaning_changes.json\"\n",
    "\n",
    "LABEL_A_ORIG = \"Bạn có tin vào tình yêu sét đánh không?\"\n",
    "LABEL_B_ORIG = \"Bạn đã từng trải qua một mối tình chính thức chưa?\"\n",
    "LABEL_A = \"label_a_love_at_first_sight\"\n",
    "LABEL_B = \"label_b_has_had_official_relationship\"\n",
    "\n",
    "# ---------- Helper ----------\n",
    "def slugify(name: str) -> str:\n",
    "    s = str(name).strip()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s)\n",
    "    return s.lower()\n",
    "\n",
    "YESNO_MAP = {\n",
    "    \"có\": 1, \"co\":1, \"yes\":1, \"y\":1, \"true\":1, \"1\":1,\n",
    "    \"không\": 0, \"khong\":0, \"ko\":0, \"no\":0, \"false\":0, \"0\":0\n",
    "}\n",
    "LIKERT_MAP = {\n",
    "    \"rất không đồng ý\": 1, \"rat khong dong y\":1,\n",
    "    \"không đồng ý\": 2, \"khong dong y\":2,\n",
    "    \"phân vân\": 3, \"phan van\":3, \"trung lập\":3,\n",
    "    \"đồng ý\": 4, \"dong y\":4,\n",
    "    \"rất đồng ý\": 5, \"rat dong y\":5\n",
    "}\n",
    "\n",
    "# ---------- Read original ----------\n",
    "print(\"Reading:\", DATA_PATH)\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"File not found: {DATA_PATH}\")\n",
    "df_orig = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "print(\"Original shape:\", df_orig.shape)\n",
    "\n",
    "# ---------- Detect cols to drop ----------\n",
    "empty_cols = df_orig.columns[df_orig.isna().all()].tolist()\n",
    "dup_mask = df_orig.T.duplicated(keep=\"first\")\n",
    "dup_cols = df_orig.columns[dup_mask].tolist()\n",
    "likely_timestamp = [c for c in df_orig.columns if re.search(r\"time|timestamp|date\", str(c), flags=re.I)]\n",
    "likely_email = [c for c in df_orig.columns if re.search(r\"email\", str(c), flags=re.I)]\n",
    "explicit_junk = [c for c in [\"Column 20\",\"Column 48\",\"Column 48.1\",\"Score\"] if c in df_orig.columns]\n",
    "\n",
    "cols_to_drop = sorted(set(empty_cols + dup_cols + likely_timestamp + likely_email + explicit_junk))\n",
    "cols_to_drop_existing = [c for c in cols_to_drop if c in df_orig.columns]\n",
    "\n",
    "print(f\"Detected to_drop: empty={len(empty_cols)}, dup={len(dup_cols)}, ts={len(likely_timestamp)}, email={len(likely_email)}, explicit_junk={len(explicit_junk)}\")\n",
    "print(\"Example to_drop (up to 20):\", cols_to_drop_existing[:20])\n",
    "\n",
    "# ---------- Create rename map and save ----------\n",
    "rename_map = {c: slugify(c) for c in df_orig.columns}\n",
    "if LABEL_A_ORIG in rename_map:\n",
    "    rename_map[LABEL_A_ORIG] = LABEL_A\n",
    "if LABEL_B_ORIG in rename_map:\n",
    "    rename_map[LABEL_B_ORIG] = LABEL_B\n",
    "\n",
    "pd.DataFrame({\"original\": list(rename_map.keys()), \"suggested\": list(rename_map.values())}) \\\n",
    "  .to_csv(COL_RENAME_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved col rename map to:\", COL_RENAME_PATH)\n",
    "\n",
    "# ---------- Build cleaned df (drop + rename) ----------\n",
    "df = df_orig.copy()\n",
    "if cols_to_drop_existing:\n",
    "    df = df.drop(columns=cols_to_drop_existing)\n",
    "print(\"After drop shape:\", df.shape)\n",
    "df = df.rename(columns={orig:rename_map.get(orig, orig) for orig in df.columns})\n",
    "print(\"After rename sample cols:\", list(df.columns)[:12])\n",
    "\n",
    "# ---------- Normalize Yes/No and Likert (safe assign only to matching cells) ----------\n",
    "for col in df.columns:\n",
    "    # operate on string/object columns and also allow existing numbers to remain\n",
    "    if df[col].dtype == object or str(df[col].dtype).startswith(\"string\"):\n",
    "        s = df[col].astype(\"string\").str.strip().str.lower()\n",
    "\n",
    "        # Yes/No mapping: only assign where exact match in YESNO_MAP\n",
    "        mask_yes = s.isin(YESNO_MAP.keys())\n",
    "        if mask_yes.any():\n",
    "            mapped = s.map(YESNO_MAP)\n",
    "            df.loc[mask_yes, col] = mapped[mask_yes].astype(\"Int64\")\n",
    "\n",
    "        # Likert mapping: only assign where exact match in LIKERT_MAP\n",
    "        s2 = df[col].astype(\"string\").str.strip().str.lower()\n",
    "        mask_lik = s2.isin(LIKERT_MAP.keys())\n",
    "        if mask_lik.any():\n",
    "            mapped_l = s2.map(LIKERT_MAP)\n",
    "            df.loc[mask_lik, col] = mapped_l[mask_lik].astype(\"Int64\")\n",
    "\n",
    "# ---------- Coerce columns that are overwhelmingly numeric to numeric dtype ----------\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[col].dtype):\n",
    "        continue\n",
    "    coerced = pd.to_numeric(df[col], errors='coerce')\n",
    "    if coerced.notna().sum() >= 0.9 * len(coerced):\n",
    "        df[col] = coerced\n",
    "\n",
    "# ---------- Identify columns for imputation (EXCLUDE LABELS) ----------\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_impute_cols = [c for c in numeric_cols if c not in (LABEL_A, LABEL_B)]\n",
    "obj_cols = [c for c in df.columns if c not in numeric_cols and c not in (LABEL_A, LABEL_B)]\n",
    "\n",
    "# ---------- Impute features only ----------\n",
    "numeric_imputed = {}\n",
    "for c in numeric_impute_cols:\n",
    "    n_miss = int(df[c].isna().sum())\n",
    "    if n_miss > 0:\n",
    "        med = df[c].median()\n",
    "        df[c] = df[c].fillna(med)\n",
    "        numeric_imputed[c] = int(n_miss)\n",
    "\n",
    "categorical_imputed = {}\n",
    "for c in obj_cols:\n",
    "    n_miss = int(df[c].isna().sum())\n",
    "    if n_miss > 0:\n",
    "        df[c] = df[c].fillna(\"Unknown\")\n",
    "        categorical_imputed[c] = int(n_miss)\n",
    "\n",
    "# ---------- Create per-label datasets: drop rows where that label is missing (NaN) ----------\n",
    "initial_rows = len(df)\n",
    "# Note: labels were never imputed -> NaN preserved; if label column absent, behave accordingly\n",
    "df_for_a = df[df[LABEL_A].notna()].copy() if LABEL_A in df.columns else df.copy()\n",
    "df_for_b = df[df[LABEL_B].notna()].copy() if LABEL_B in df.columns else df.copy()\n",
    "dropped_for_a = initial_rows - len(df_for_a)\n",
    "dropped_for_b = initial_rows - len(df_for_b)\n",
    "\n",
    "# ---------- Save outputs ----------\n",
    "df.to_csv(CLEAN_V1_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "df_for_a.to_csv(CLEAN_A_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "df_for_b.to_csv(CLEAN_B_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "changes = {\n",
    "    \"initial_rows\": int(len(df_orig)),\n",
    "    \"after_drop_rename_rows\": int(len(df)),\n",
    "    \"after_drop_rename_cols\": int(len(df.columns)),\n",
    "    \"cols_dropped\": cols_to_drop_existing,\n",
    "    \"numeric_imputed_counts\": numeric_imputed,\n",
    "    \"categorical_imputed_counts\": categorical_imputed,\n",
    "    \"rows_dropped_for_labelA\": int(dropped_for_a),\n",
    "    \"rows_dropped_for_labelB\": int(dropped_for_b),\n",
    "    \"col_rename_map_path\": str(COL_RENAME_PATH),\n",
    "    \"clean_v1_path\": str(CLEAN_V1_PATH),\n",
    "    \"clean_for_labelA_path\": str(CLEAN_A_PATH),\n",
    "    \"clean_for_labelB_path\": str(CLEAN_B_PATH)\n",
    "}\n",
    "with open(CHANGES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(changes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---------- Summary ----------\n",
    "print(\"\\nPHASE B (fixed) COMPLETE.\")\n",
    "print(f\"Cleaned primary dataset saved: {CLEAN_V1_PATH}  (rows={len(df)}, cols={len(df.columns)})\")\n",
    "print(f\"Per-label datasets saved: labelA -> {CLEAN_A_PATH} (rows={len(df_for_a)}), labelB -> {CLEAN_B_PATH} (rows={len(df_for_b)})\")\n",
    "print(f\"Rename map saved: {COL_RENAME_PATH}\")\n",
    "print(f\"Cleaning changes saved: {CHANGES_PATH}\")\n",
    "print(\"\\nDropped columns count:\", len(cols_to_drop_existing))\n",
    "print(\"Numeric columns imputed (count by col):\", numeric_imputed)\n",
    "print(\"Categorical columns imputed (count by col):\", categorical_imputed)\n",
    "print(\"Rows dropped for labelA:\", int(dropped_for_a), \"Rows dropped for labelB:\", int(dropped_for_b))\n",
    "print(\"\\nIf you want labels imputed instead of dropping rows, tell me and I will change strategy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03550038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\ml\\love-survey-ml\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset: data\\clean_data_v1.csv\n",
      "Shape: (208, 42)\n",
      "Columns: 42\n",
      "EDA summary saved to: reports\\eda_summary.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ProfileReport (this may take ~10–20 seconds)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 142.32it/s]<00:00, 54.58it/s, Describe variable: bạn_cảm_thấy_gia_đình_có_ảnh_hưởng_đến_mối_quan_hệ_của_bạn_như_thế_nào]                                                                      \n",
      "Summarize dataset: 100%|██████████| 51/51 [00:03<00:00, 13.71it/s, Completed]                                                                                \n",
      "Generate report structure: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 152.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProfileReport saved to: reports\\eda_report.html\n",
      "\n",
      "PHASE C COMPLETE — EDA + Profile Report created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== PHASE C (FULL) — Exploratory Data Analysis (EDA + ProfileReport) =====\n",
    "# 1 PHASE = 1 cell — theo yêu cầu của bạn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------\n",
    "DATA_PATH = Path(\"data/clean_data_v1.csv\")   # đã được tạo từ Phase B\n",
    "REPORT_DIR = Path(\"reports\")\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HTML_REPORT = REPORT_DIR / \"eda_report.html\"\n",
    "EDA_SUMMARY_PATH = REPORT_DIR / \"eda_summary.csv\"\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# --------------------------------------------------------------\n",
    "print(\"Loading cleaned dataset:\", DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH, encoding=\"utf-8-sig\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", len(df.columns))\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# BASIC SUMMARY (save to CSV)\n",
    "# --------------------------------------------------------------\n",
    "summary = pd.DataFrame({\n",
    "    \"column\": df.columns,\n",
    "    \"dtype\": df.dtypes.astype(str).values,\n",
    "    \"n_missing\": df.isna().sum().values,\n",
    "    \"unique_values\": df.nunique().values\n",
    "})\n",
    "summary.to_csv(EDA_SUMMARY_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"EDA summary saved to:\", EDA_SUMMARY_PATH)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# DISTRIBUTION CHARTS (LABEL A / LABEL B)\n",
    "# --------------------------------------------------------------\n",
    "label_a = \"label_a_love_at_first_sight\"\n",
    "label_b = \"label_b_has_had_official_relationship\"\n",
    "\n",
    "if label_a in df.columns:\n",
    "    plt.figure(figsize=(4,3))\n",
    "    df[label_a].value_counts(dropna=False).sort_index().plot(kind=\"bar\")\n",
    "    plt.title(\"Distribution of Label A\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if label_b in df.columns:\n",
    "    plt.figure(figsize=(4,3))\n",
    "    df[label_b].value_counts(dropna=False).sort_index().plot(kind=\"bar\")\n",
    "    plt.title(\"Distribution of Label B\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# PROFILE REPORT (HTML) — REPORT GIỐNG MẪU CỦA BẠN\n",
    "# --------------------------------------------------------------\n",
    "print(\"Generating ProfileReport (this may take ~10–20 seconds)...\")\n",
    "\n",
    "profile = ProfileReport(\n",
    "    df,\n",
    "    title=\"Love Survey — EDA Profile Report\",\n",
    "    explorative=True,\n",
    "    sensitive=False,\n",
    ")\n",
    "\n",
    "profile.to_file(HTML_REPORT)\n",
    "print(\"ProfileReport saved to:\", HTML_REPORT)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# DONE\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nPHASE C COMPLETE — EDA + Profile Report created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba807137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE D — Feature Preparation starting...\n",
      "Loading cleaned data: data\\clean_data_v1.csv\n",
      "Loaded shape: (208, 42)\n",
      "Detected numeric cols: 10\n",
      "Detected categorical/object cols: 31\n",
      "Detected likert-like cols (heuristic): ['độ_tuổi_của_bạn_là', 'trình_độ_học_vấn_hiện_tại_của_bạn', 'mức_độ_hài_lòng_của_bạn_về_mối_quan_hệ_hiện_tại', 'bạn_và_người_yêu_có_thường_xuyên_xung_đột_không', 'nếu_bạn_cảm_thấy_người_yêu_của_mình_quá_gần_gũi_người_khác_mức_độ_ghen_tị_của_bạn_ở_đâu', 'tình_yêu_ảnh_hưởng_đến_kết_quả_học_tập_sự_nghiệp_ở_mức_độ_như_thế_nào']\n",
      "Composite scores added: ['romantic_score', 'social_pressure_index', 'jealousy_index', 'communication_score']\n",
      "Romantic cols used: ['tình_yêu_ảnh_hưởng_đến_kết_quả_học_tập_sự_nghiệp_ở_mức_độ_như_thế_nào']\n",
      "Social pressure cols used: ['bạn_có_cảm_thấy_áp_lực_từ_gia_đình_hoặc_xã_hội_về_việc_phải_có_người_yêu_không', 'có_nên_chia_sẻ_thường_xuyên_chuyện_hẹn_hò_cho_gia_đình_không', 'bạn_cảm_thấy_gia_đình_có_ảnh_hưởng_đến_mối_quan_hệ_của_bạn_như_thế_nào']\n",
      "Jealousy cols used: ['nếu_bạn_cảm_thấy_người_yêu_của_mình_quá_gần_gũi_người_khác_mức_độ_ghen_tị_của_bạn_ở_đâu']\n",
      "Communication cols used: ['có_nên_chia_sẻ_thường_xuyên_chuyện_hẹn_hò_cho_gia_đình_không']\n",
      "Categorical columns total: 31\n",
      "One-hot columns (<=10 unique): 23\n",
      "Frequency-encoded columns: 8\n",
      "Fitting preprocessor on features shape: (208, 44)\n",
      "Transformed features shape: (208, 115)\n",
      "Top features for Label A (count=20):\n",
      "Top features for Label B (count=20):\n",
      "Final selected features count: 25\n",
      "Saved feature matrix to: data\\X_features.csv\n",
      "Saved preprocessor pipeline to: data\\feature_pipeline.pkl\n",
      "Saved feature info to: data\\feature_info.json\n",
      "\n",
      "PHASE D COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "# ===== PHASE D — FEATURE PREPARATION (1 cell = toàn Phase D) =====\n",
    "# Outputs (saved to data/):\n",
    "# - data/X_features.csv                (feature matrix for full dataset)\n",
    "# - data/feature_pipeline.pkl          (preprocessing pipeline)\n",
    "# - data/feature_info.json             (metadata: selected features, mapping)\n",
    "# Notes: Reads data/clean_data_v1.csv and data/clean_for_labelA.csv / clean_for_labelB.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import joblib\n",
    "\n",
    "# -------- CONFIG --------\n",
    "DATA_DIR = Path(\"data\")\n",
    "CLEAN_PATH = DATA_DIR / \"clean_data_v1.csv\"\n",
    "CLEAN_A = DATA_DIR / \"clean_for_labelA.csv\"\n",
    "CLEAN_B = DATA_DIR / \"clean_for_labelB.csv\"\n",
    "OUT_X = DATA_DIR / \"X_features.csv\"\n",
    "PIPE_OUT = DATA_DIR / \"feature_pipeline.pkl\"\n",
    "INFO_OUT = DATA_DIR / \"feature_info.json\"\n",
    "\n",
    "# thresholds / params\n",
    "oh_threshold = 10          # cardinality threshold for one-hot encoding\n",
    "top_k = 20                 # number of top features to keep per-label for RF selection\n",
    "random_state = 42\n",
    "\n",
    "print(\"PHASE D — Feature Preparation starting...\")\n",
    "print(\"Loading cleaned data:\", CLEAN_PATH)\n",
    "df = pd.read_csv(CLEAN_PATH, encoding=\"utf-8-sig\")\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "\n",
    "# LABEL NAMES (standardized earlier)\n",
    "label_a = \"label_a_love_at_first_sight\"\n",
    "label_b = \"label_b_has_had_official_relationship\"\n",
    "\n",
    "# ---- helper: detect column types ----\n",
    "def is_likert_like(col_name):\n",
    "    # heuristic: contains keywords often in Likert questions\n",
    "    kws = [\"đồng ý\", \"đồng_y\", \"rất\", \"hài_lòng\", \"mức_độ\", \"độ\", \"độ_ghen\", \"mức_độ\"]\n",
    "    cn = str(col_name).lower()\n",
    "    return any(k in cn for k in kws)\n",
    "\n",
    "def find_cols_by_keywords(df_cols, keywords):\n",
    "    cols = []\n",
    "    for c in df_cols:\n",
    "        low = str(c).lower()\n",
    "        if any(k in low for k in keywords):\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "# ---- identify numeric vs categorical vs object columns ----\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "obj_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "# Exclude labels from feature lists\n",
    "features_all = [c for c in df.columns if c not in (label_a, label_b)]\n",
    "\n",
    "# Determine categorical candidates among obj_cols (string-like)\n",
    "cat_candidates = [c for c in obj_cols if c in features_all]\n",
    "# Determine likely likert (if any still object)\n",
    "likert_cols = [c for c in features_all if is_likert_like(c)]\n",
    "# But also check numeric columns that may be ordinal already\n",
    "likert_cols = list(dict.fromkeys(likert_cols))  # unique\n",
    "\n",
    "print(\"Detected numeric cols:\", len(numeric_cols))\n",
    "print(\"Detected categorical/object cols:\", len(cat_candidates))\n",
    "print(\"Detected likert-like cols (heuristic):\", likert_cols)\n",
    "\n",
    "# ---- ORDINAL MAPPING for common Likert strings (if present) ----\n",
    "likert_map = {\n",
    "    \"rất không đồng ý\": 1, \"rat_khong_dong_y\":1, \"rat khong dong y\":1,\n",
    "    \"không đồng ý\": 2, \"khong_dong_y\":2, \"khong dong y\":2,\n",
    "    \"phân vân\": 3, \"phan van\":3, \"trung lập\":3, \"trung lap\":3,\n",
    "    \"đồng ý\": 4, \"dong y\":4,\n",
    "    \"rất đồng ý\": 5, \"rat dong y\":5\n",
    "}\n",
    "\n",
    "def map_likert_series(s):\n",
    "    # convert to string lowercase and map if possible; else leave\n",
    "    s2 = s.astype(\"string\").str.strip().str.lower().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    mapped = s2.map(lambda x: likert_map.get(x, np.nan))\n",
    "    # if many mapped values, return mapped else return original series\n",
    "    if mapped.notna().sum() > 0:\n",
    "        return mapped\n",
    "    return s\n",
    "\n",
    "# Apply ordinal mapping to object-like likert candidates\n",
    "for c in likert_cols:\n",
    "    if c in df.columns:\n",
    "        mapped = map_likert_series(df[c])\n",
    "        # if mapped produced numeric values for some cells, assign them (keep rest as-is)\n",
    "        if mapped.notna().sum() > 0:\n",
    "            df.loc[mapped.notna(), c] = mapped[mapped.notna()]\n",
    "            # coerce to numeric if majority numeric\n",
    "            coerced = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if coerced.notna().sum() >= 0.5 * len(coerced):\n",
    "                df[c] = coerced\n",
    "\n",
    "# Refresh numeric / categorical sets after mapping\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_candidates = [c for c in df.columns if c not in numeric_cols and c not in (label_a, label_b)]\n",
    "\n",
    "# Remove any unnatural large-text columns from features (free text)\n",
    "# Heuristic: cardinality > 0.5 * n_rows and dtype object -> treat as free text and drop from modeling features\n",
    "n_rows = len(df)\n",
    "free_text_cols = [c for c in cat_candidates if df[c].nunique(dropna=False) > 0.5 * n_rows]\n",
    "if free_text_cols:\n",
    "    print(\"Detected high-cardinality free-text columns (will be excluded from features):\", free_text_cols)\n",
    "# final feature list excluding free-text\n",
    "feature_candidates = [c for c in features_all if c not in free_text_cols and c not in (label_a, label_b)]\n",
    "\n",
    "# ---- Feature engineering: composite scores (heuristic using VN keywords) ----\n",
    "# Define keyword groups\n",
    "romantic_kw = [\"tình_yêu\", \"tinh_yeu\", \"tình yêu\", \"yeu\", \"romantic\", \"romance\"]\n",
    "social_pressure_kw = [\"áp_lực\", \"ap_luc\", \"gia_đình\", \"gia_dinh\", \"áp lực\", \"gia đình\"]\n",
    "jealousy_kw = [\"ghen\", \"ghen_tị\", \"ghen_ti\"]\n",
    "communication_kw = [\"chia_sẻ\", \"chia_se\", \"chia sẻ\", \"giao tiếp\", \"giao_tiep\", \"communication\", \"truyền thông\"]\n",
    "\n",
    "def make_composite(df, kws):\n",
    "    cols = find_cols_by_keywords(df.columns, kws)\n",
    "    cols = [c for c in cols if c in df.columns and np.issubdtype(df[c].dtype, np.number)]\n",
    "    if not cols:\n",
    "        return None, []\n",
    "    comp = df[cols].mean(axis=1)\n",
    "    return comp, cols\n",
    "\n",
    "rom_score, rom_cols = make_composite(df, romantic_kw)\n",
    "soc_score, soc_cols = make_composite(df, social_pressure_kw)\n",
    "jez_score, jez_cols = make_composite(df, jealousy_kw)\n",
    "com_score, com_cols = make_composite(df, communication_kw)\n",
    "\n",
    "# attach composite features (if any)\n",
    "added = []\n",
    "if rom_score is not None:\n",
    "    df[\"romantic_score\"] = rom_score\n",
    "    added.append(\"romantic_score\")\n",
    "if soc_score is not None:\n",
    "    df[\"social_pressure_index\"] = soc_score\n",
    "    added.append(\"social_pressure_index\")\n",
    "if jez_score is not None:\n",
    "    df[\"jealousy_index\"] = jez_score\n",
    "    added.append(\"jealousy_index\")\n",
    "if com_score is not None:\n",
    "    df[\"communication_score\"] = com_score\n",
    "    added.append(\"communication_score\")\n",
    "\n",
    "print(\"Composite scores added:\", added)\n",
    "print(\"Romantic cols used:\", rom_cols)\n",
    "print(\"Social pressure cols used:\", soc_cols)\n",
    "print(\"Jealousy cols used:\", jez_cols)\n",
    "print(\"Communication cols used:\", com_cols)\n",
    "\n",
    "# update feature candidates to include new composites\n",
    "feature_candidates = [c for c in feature_candidates if c not in (label_a, label_b)]\n",
    "feature_candidates += added\n",
    "\n",
    "# ---- Encoding strategy: decide which categorical cols get one-hot vs freq-encoding ----\n",
    "cat_cols = [c for c in feature_candidates if c in cat_candidates]\n",
    "oh_cols = [c for c in cat_cols if df[c].nunique(dropna=False) <= oh_threshold]\n",
    "freq_cols = [c for c in cat_cols if c not in oh_cols]\n",
    "\n",
    "print(\"Categorical columns total:\", len(cat_cols))\n",
    "print(\"One-hot columns (<=%d unique): %d\" % (oh_threshold, len(oh_cols)))\n",
    "print(\"Frequency-encoded columns: %d\" % len(freq_cols))\n",
    "\n",
    "# ---- Build feature processing pipeline ----\n",
    "# Frequency encoder implemented as a simple transformer\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "        self.maps_ = {}\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        for c in self.cols:\n",
    "            vc = X[c].fillna(\"__MISSING__\").value_counts(normalize=True)\n",
    "            self.maps_[c] = vc.to_dict()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for c in self.cols:\n",
    "            X[c] = X[c].fillna(\"__MISSING__\").map(self.maps_.get(c, {})).astype(float)\n",
    "            X[c] = X[c].fillna(0.0)\n",
    "        return X[self.cols]\n",
    "\n",
    "# Column lists for pipeline\n",
    "num_cols = [c for c in feature_candidates if c in numeric_cols and c not in (label_a, label_b)]\n",
    "oh_cols = [c for c in oh_cols if c in feature_candidates]\n",
    "freq_cols = [c for c in freq_cols if c in feature_candidates]\n",
    "\n",
    "# numeric pipeline: impute median + scale\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# safe OneHotEncoder factory: try old param first, fallback to new param\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def make_ohe(handle_unknown=\"ignore\"):\n",
    "    try:\n",
    "        # older sklearn: OneHotEncoder(sparse=False)\n",
    "        return OneHotEncoder(handle_unknown=handle_unknown, sparse=False)\n",
    "    except TypeError:\n",
    "        # newer sklearn: OneHotEncoder(sparse_output=False)\n",
    "        return OneHotEncoder(handle_unknown=handle_unknown, sparse_output=False)\n",
    "\n",
    "ohe_est = make_ohe(handle_unknown=\"ignore\")\n",
    "\n",
    "oh_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "    (\"ohe\", ohe_est)\n",
    "])\n",
    "\n",
    "\n",
    "# freq pipeline\n",
    "freq_pipeline = Pipeline([\n",
    "    (\"freq\", FrequencyEncoder(cols=freq_cols))\n",
    "])\n",
    "\n",
    "# Compose ColumnTransformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_pipeline, num_cols))\n",
    "if oh_cols:\n",
    "    transformers.append((\"ohe\", oh_pipeline, oh_cols))\n",
    "if freq_cols:\n",
    "    transformers.append((\"freq\", freq_pipeline, freq_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=0)\n",
    "\n",
    "# Fit preprocessor on full df (features present)\n",
    "X_for_preproc = df[feature_candidates].copy()\n",
    "print(\"Fitting preprocessor on features shape:\", X_for_preproc.shape)\n",
    "preprocessor.fit(X_for_preproc)\n",
    "\n",
    "# Transform to get feature matrix for entire dataset\n",
    "X_trans = preprocessor.transform(X_for_preproc)\n",
    "# Build feature names for transformed matrix\n",
    "feature_names = []\n",
    "if num_cols:\n",
    "    feature_names += num_cols\n",
    "if oh_cols:\n",
    "    # get categories\n",
    "    ohe = preprocessor.named_transformers_[\"ohe\"].named_steps[\"ohe\"]\n",
    "    oh_cat_names = ohe.get_feature_names_out(oh_cols).tolist()\n",
    "    feature_names += oh_cat_names\n",
    "if freq_cols:\n",
    "    feature_names += freq_cols\n",
    "\n",
    "X_df = pd.DataFrame(X_trans, columns=feature_names, index=df.index)\n",
    "print(\"Transformed features shape:\", X_df.shape)\n",
    "\n",
    "# ---- Feature selection using RandomForest importances for both labels ----\n",
    "selected_features = set()\n",
    "def rf_select_features(X, y, k=top_k):\n",
    "    # coerce X to numeric matrix\n",
    "    # fill NaN with median\n",
    "    X2 = X.copy().fillna(X.median())\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=random_state, n_jobs=-1, class_weight=\"balanced\")\n",
    "    clf.fit(X2, y)\n",
    "    importances = pd.Series(clf.feature_importances_, index=X2.columns).sort_values(ascending=False)\n",
    "    top = importances.head(k).index.tolist()\n",
    "    return top, importances\n",
    "\n",
    "# For label A\n",
    "if label_a in df.columns:\n",
    "    df_a = pd.read_csv(CLEAN_A, encoding=\"utf-8-sig\") if Path(CLEAN_A).exists() else df[df[label_a].notna()].copy()\n",
    "    # align columns with processed X_df\n",
    "    idx_a = df_a.index\n",
    "    X_a = X_df.loc[idx_a].copy()\n",
    "    y_a = df_a[label_a]\n",
    "    if y_a.isna().sum() > 0:\n",
    "        y_a = y_a.dropna()\n",
    "        X_a = X_a.loc[y_a.index]\n",
    "    if len(y_a.unique()) > 1 and len(X_a) > 5:\n",
    "        top_a, imp_a = rf_select_features(X_a, y_a, k=top_k)\n",
    "        print(\"Top features for Label A (count=%d):\" % len(top_a))\n",
    "        selected_features.update(top_a)\n",
    "    else:\n",
    "        print(\"Skipping RF selection for Label A (insufficient variance or rows).\")\n",
    "\n",
    "# For label B\n",
    "if label_b in df.columns:\n",
    "    df_b = pd.read_csv(CLEAN_B, encoding=\"utf-8-sig\") if Path(CLEAN_B).exists() else df[df[label_b].notna()].copy()\n",
    "    idx_b = df_b.index\n",
    "    X_b = X_df.loc[idx_b].copy()\n",
    "    y_b = df_b[label_b]\n",
    "    if y_b.isna().sum() > 0:\n",
    "        y_b = y_b.dropna()\n",
    "        X_b = X_b.loc[y_b.index]\n",
    "    if len(y_b.unique()) > 1 and len(X_b) > 5:\n",
    "        top_b, imp_b = rf_select_features(X_b, y_b, k=top_k)\n",
    "        print(\"Top features for Label B (count=%d):\" % len(top_b))\n",
    "        selected_features.update(top_b)\n",
    "    else:\n",
    "        print(\"Skipping RF selection for Label B (insufficient variance or rows).\")\n",
    "\n",
    "# Fallback: if selection empty, use all features\n",
    "if not selected_features:\n",
    "    print(\"No selected features from RF; using all transformed features as final features.\")\n",
    "    final_features = X_df.columns.tolist()\n",
    "else:\n",
    "    final_features = [c for c in X_df.columns if c in selected_features]\n",
    "    # ensure at least some features exist\n",
    "    if len(final_features) < 5:\n",
    "        add_more = [c for c in X_df.columns if c not in final_features][:20]\n",
    "        final_features += add_more\n",
    "\n",
    "print(\"Final selected features count:\", len(final_features))\n",
    "\n",
    "# Save feature matrix (full) but only keep final features\n",
    "X_final = X_df[final_features].copy()\n",
    "X_final.to_csv(OUT_X, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved feature matrix to:\", OUT_X)\n",
    "\n",
    "# Save pipeline and metadata\n",
    "joblib.dump(preprocessor, PIPE_OUT)\n",
    "print(\"Saved preprocessor pipeline to:\", PIPE_OUT)\n",
    "\n",
    "info = {\n",
    "    \"num_cols\": num_cols,\n",
    "    \"onehot_cols\": oh_cols,\n",
    "    \"freq_cols\": freq_cols,\n",
    "    \"free_text_cols_excluded\": free_text_cols,\n",
    "    \"composite_scores_added\": added,\n",
    "    \"final_features\": final_features,\n",
    "    \"n_rows\": int(n_rows)\n",
    "}\n",
    "with open(INFO_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(info, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved feature info to:\", INFO_OUT)\n",
    "\n",
    "print(\"\\nPHASE D COMPLETE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e33b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE E — Train/Test Setup starting...\n",
      "Loading feature matrix: data\\X_features.csv\n",
      "Features loaded: rows=208, cols=25\n",
      "Label A found: True\n",
      "Label B found: True\n",
      "Label B has only one class ([1.]). Doing random split without stratify.\n",
      "imblearn.SMO TE available — will create SMOTE oversampled train for Label B (if B exists).\n",
      "SMOTE failed with error: Input y contains NaN.\n",
      "\n",
      "PHASE E COMPLETE.\n",
      "Saved train.pkl -> data\\train.pkl\n",
      "Saved test.pkl -> data\\test.pkl\n",
      "Saved split info -> data\\split_info.json\n",
      "Also saved CSV copies: train_X_*.csv, train_y_*.csv, test_X_*.csv, test_y_*.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\ml\\love-survey-ml\\venv\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\numpy\\_aliases.py:125: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype=dtype, copy=copy)\n"
     ]
    }
   ],
   "source": [
    "# ===== PHASE E — TRAIN/TEST SETUP (1 cell = toàn Phase E) =====\n",
    "# - Read features: data/X_features.csv\n",
    "# - Read labels: data/clean_for_labelA.csv & data/clean_for_labelB.csv (these contain labels aligned to original indexes)\n",
    "# - Outputs saved to data/: train.pkl, test.pkl, split_info.json\n",
    "# - Attempts to create SMOTE oversampled training for Label B if imblearn available\n",
    "# - Stratify split per-label, test_size=0.2, random_state=42\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_DIR = Path(\"data\")\n",
    "X_PATH = DATA_DIR / \"X_features.csv\"\n",
    "CLEAN_V1 = DATA_DIR / \"clean_data_v1.csv\"\n",
    "CLEAN_A = DATA_DIR / \"clean_for_labelA.csv\"\n",
    "CLEAN_B = DATA_DIR / \"clean_for_labelB.csv\"\n",
    "\n",
    "TRAIN_OUT = DATA_DIR / \"train.pkl\"\n",
    "TEST_OUT = DATA_DIR / \"test.pkl\"\n",
    "SPLIT_INFO = DATA_DIR / \"split_info.json\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "print(\"PHASE E — Train/Test Setup starting...\")\n",
    "print(\"Loading feature matrix:\", X_PATH)\n",
    "if not X_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected features at {X_PATH}. Please run Phase D first.\")\n",
    "\n",
    "X_full = pd.read_csv(X_PATH, encoding=\"utf-8-sig\")\n",
    "n_rows, n_cols = X_full.shape\n",
    "print(f\"Features loaded: rows={n_rows}, cols={n_cols}\")\n",
    "\n",
    "# Load labels from cleaned datasets (these keep original indexing in Phase B)\n",
    "# Prefer reading the per-label cleaned files (they preserve original indices)\n",
    "labels_a = None\n",
    "labels_b = None\n",
    "\n",
    "if CLEAN_A.exists():\n",
    "    df_a = pd.read_csv(CLEAN_A, encoding=\"utf-8-sig\")\n",
    "    # align indices - if the clean files were created from original df, they may have contiguous index.\n",
    "    # We'll assume index alignment with X_features: if not, fallback to using CLEAN_V1.\n",
    "    labels_a = df_a.get(\"label_a_love_at_first_sight\") if \"label_a_love_at_first_sight\" in df_a.columns else None\n",
    "\n",
    "if labels_a is None and Path(CLEAN_V1).exists():\n",
    "    df_v1 = pd.read_csv(CLEAN_V1, encoding=\"utf-8-sig\")\n",
    "    labels_a = df_v1.get(\"label_a_love_at_first_sight\") if \"label_a_love_at_first_sight\" in df_v1.columns else None\n",
    "\n",
    "if CLEAN_B.exists():\n",
    "    df_b = pd.read_csv(CLEAN_B, encoding=\"utf-8-sig\")\n",
    "    labels_b = df_b.get(\"label_b_has_had_official_relationship\") if \"label_b_has_had_official_relationship\" in df_b.columns else None\n",
    "\n",
    "if labels_b is None and Path(CLEAN_V1).exists():\n",
    "    df_v1 = pd.read_csv(CLEAN_V1, encoding=\"utf-8-sig\")\n",
    "    labels_b = df_v1.get(\"label_b_has_had_official_relationship\") if \"label_b_has_had_official_relationship\" in df_v1.columns else None\n",
    "\n",
    "# Quick checks\n",
    "print(\"Label A found:\", labels_a is not None)\n",
    "print(\"Label B found:\", labels_b is not None)\n",
    "\n",
    "# ------------------ REPLACE build_xy_for_label WITH THIS SAFE VERSION ------------------\n",
    "def build_xy_for_label(X, label_series, label_name):\n",
    "    \"\"\"\n",
    "    Safe alignment: reset indices for both X and y to positional alignment,\n",
    "    then drop rows with missing labels.\n",
    "    Returns X_valid (DataFrame), y_valid (Series) aligned by position.\n",
    "    \"\"\"\n",
    "    if label_series is None:\n",
    "        print(f\"Warning: label {label_name} not found. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    # Reset indices for positional alignment (best-effort)\n",
    "    X_reset = X.reset_index(drop=True).copy()\n",
    "    y_reset = label_series.reset_index(drop=True).copy()\n",
    "\n",
    "    # If label length shorter/longer than X, trim or pad with NaN to match length\n",
    "    if len(y_reset) < len(X_reset):\n",
    "        # pad y with NaN to match length\n",
    "        pad_len = len(X_reset) - len(y_reset)\n",
    "        y_reset = pd.concat([y_reset, pd.Series([np.nan]*pad_len)], ignore_index=True)\n",
    "    elif len(y_reset) > len(X_reset):\n",
    "        # truncate y to X length\n",
    "        y_reset = y_reset.iloc[:len(X_reset)].reset_index(drop=True)\n",
    "\n",
    "    # Now drop rows where label is missing\n",
    "    mask_notna = y_reset.notna()\n",
    "    if mask_notna.sum() == 0:\n",
    "        print(f\"Warning: After alignment, label {label_name} has zero non-missing rows.\")\n",
    "        return None, None\n",
    "\n",
    "    X_valid = X_reset.loc[mask_notna].reset_index(drop=True)\n",
    "    y_valid = y_reset.loc[mask_notna].reset_index(drop=True)\n",
    "\n",
    "    # Try to coerce y to numeric if possible\n",
    "    try:\n",
    "        y_valid = pd.to_numeric(y_valid, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return X_valid, y_valid\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Build datasets for Label A and Label B\n",
    "X_a, y_a = build_xy_for_label(X_full, labels_a, \"Label A\")\n",
    "X_b, y_b = build_xy_for_label(X_full, labels_b, \"Label B\")\n",
    "\n",
    "# Containers for outputs\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "split_summary = {\n",
    "    \"random_state\": int(RANDOM_STATE),\n",
    "    \"test_size\": float(TEST_SIZE),\n",
    "    \"labelA\": {},\n",
    "    \"labelB\": {}\n",
    "}\n",
    "\n",
    "# Function to split and store\n",
    "def stratified_split_and_store(X, y, label_key):\n",
    "    if X is None or y is None:\n",
    "        print(f\"Skipping {label_key} (missing X or y).\")\n",
    "        return None, None, {}\n",
    "    # Ensure at least 2 classes to stratify; if single class, do random split without stratify\n",
    "    unique_vals = y.dropna().unique()\n",
    "    if len(unique_vals) < 2:\n",
    "        print(f\"Label {label_key} has only one class ({unique_vals}). Doing random split without stratify.\")\n",
    "        stratify_param = None\n",
    "    else:\n",
    "        stratify_param = y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=stratify_param\n",
    "    )\n",
    "    info = {\n",
    "        \"n_total\": int(len(X)),\n",
    "        \"n_train\": int(len(X_train)),\n",
    "        \"n_test\": int(len(X_test)),\n",
    "        \"classes\": list(map(int, np.unique(y.dropna()))) if len(np.unique(y.dropna()))>0 else [],\n",
    "        \"class_counts_full\": dict(y.value_counts(dropna=False).to_dict())\n",
    "    }\n",
    "    return (X_train, y_train), (X_test, y_test), info\n",
    "\n",
    "# Split for A\n",
    "if X_a is not None:\n",
    "    (X_train_a, y_train_a), (X_test_a, y_test_a), info_a = stratified_split_and_store(X_a, y_a, \"A\")\n",
    "    if X_train_a is not None:\n",
    "        train_dict[\"A\"] = {\"X\": X_train_a, \"y\": y_train_a}\n",
    "        test_dict[\"A\"] = {\"X\": X_test_a, \"y\": y_test_a}\n",
    "        split_summary[\"labelA\"] = info_a\n",
    "\n",
    "# Split for B\n",
    "if X_b is not None:\n",
    "    (X_train_b, y_train_b), (X_test_b, y_test_b), info_b = stratified_split_and_store(X_b, y_b, \"B\")\n",
    "    if X_train_b is not None:\n",
    "        train_dict[\"B\"] = {\"X\": X_train_b, \"y\": y_train_b}\n",
    "        test_dict[\"B\"] = {\"X\": X_test_b, \"y\": y_test_b}\n",
    "        split_summary[\"labelB\"] = info_b\n",
    "\n",
    "# ---------- Attempt SMOTE on Label B training only (if imblearn available) ----------\n",
    "smote_available = False\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote_available = True\n",
    "    print(\"imblearn.SMO TE available — will create SMOTE oversampled train for Label B (if B exists).\")\n",
    "except Exception as e:\n",
    "    print(\"imblearn not available (SMOTE skipped). To enable SMOTE install imbalanced-learn (pip install imbalanced-learn).\")\n",
    "\n",
    "if smote_available and \"B\" in train_dict:\n",
    "    Xb = train_dict[\"B\"][\"X\"]\n",
    "    yb = train_dict[\"B\"][\"y\"]\n",
    "    # need numeric matrix for SMOTE\n",
    "    Xb_num = Xb.fillna(Xb.median()).values\n",
    "    try:\n",
    "        sm = SMOTE(random_state=RANDOM_STATE)\n",
    "        X_res, y_res = sm.fit_resample(Xb_num, yb.values)\n",
    "        # convert back to dataframe with original columns\n",
    "        X_res_df = pd.DataFrame(X_res, columns=Xb.columns)\n",
    "        y_res_ser = pd.Series(y_res)\n",
    "        train_dict[\"B_smote\"] = {\"X\": X_res_df, \"y\": y_res_ser}\n",
    "        split_summary[\"labelB\"][\"smote\"] = {\n",
    "            \"n_train_before\": int(len(Xb)),\n",
    "            \"n_train_after\": int(len(X_res_df))\n",
    "        }\n",
    "        print(\"SMOTE applied: before=%d after=%d\" % (len(Xb), len(X_res_df)))\n",
    "    except Exception as e:\n",
    "        print(\"SMOTE failed with error:\", str(e))\n",
    "\n",
    "# ---------- Save outputs: use joblib to dump dicts (X and y as dataframes/series inside) ----------\n",
    "joblib.dump(train_dict, TRAIN_OUT)\n",
    "joblib.dump(test_dict, TEST_OUT)\n",
    "\n",
    "# Also save CSV copies for convenience\n",
    "for k,v in train_dict.items():\n",
    "    Xout = DATA_DIR / f\"train_X_{k}.csv\"\n",
    "    yout = DATA_DIR / f\"train_y_{k}.csv\"\n",
    "    v[\"X\"].to_csv(Xout, index=False, encoding=\"utf-8-sig\")\n",
    "    v[\"y\"].to_csv(yout, index=False, encoding=\"utf-8-sig\")\n",
    "for k,v in test_dict.items():\n",
    "    Xout = DATA_DIR / f\"test_X_{k}.csv\"\n",
    "    yout = DATA_DIR / f\"test_y_{k}.csv\"\n",
    "    v[\"X\"].to_csv(Xout, index=False, encoding=\"utf-8-sig\")\n",
    "    v[\"y\"].to_csv(yout, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Save split summary\n",
    "with open(SPLIT_INFO, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(split_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nPHASE E COMPLETE.\")\n",
    "print(\"Saved train.pkl ->\", TRAIN_OUT)\n",
    "print(\"Saved test.pkl ->\", TEST_OUT)\n",
    "print(\"Saved split info ->\", SPLIT_INFO)\n",
    "print(\"Also saved CSV copies: train_X_*.csv, train_y_*.csv, test_X_*.csv, test_y_*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75b8090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing train.pkl ...\n",
      "Keys in train_dict: ['A', 'B']\n",
      "Original B train shape: (117, 25)\n",
      "Original B y: total rows = 117\n",
      "Value counts (including NaN):\n",
      "NaN    80\n",
      "1.0    37\n",
      "Name: count, dtype: int64\n",
      "Dropping 80 rows from train B where y is NaN.\n",
      "After drop: Xb_clean shape: (37, 25)\n",
      "After drop: yb_clean value counts:\n",
      "1.0    37\n",
      "Name: count, dtype: int64\n",
      "Unique classes in cleaned y (excluding NaN): [1.0]\n",
      "\n",
      "==> Cannot apply SMOTE: only one class present after cleaning.\n",
      "Options now: 1) use class_weight='balanced' during training; 2) collect/label more data for minority class; 3) define another target or relabel.\n",
      "Saved cleaned train_dict (B without NaN) back to data\\train.pkl\n",
      "Done. Please paste the output here so I can verify. \n"
     ]
    }
   ],
   "source": [
    "# ===== Fix SMOTE attempt: clean Label B y (drop NaN), apply SMOTE only if >=2 classes =====\n",
    "import joblib, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "TRAIN_OUT = DATA_DIR / \"train.pkl\"\n",
    "TEST_OUT = DATA_DIR / \"test.pkl\"\n",
    "SPLIT_INFO = DATA_DIR / \"split_info.json\"\n",
    "\n",
    "print(\"Loading existing train.pkl ...\")\n",
    "train_dict = joblib.load(TRAIN_OUT) if TRAIN_OUT.exists() else {}\n",
    "print(\"Keys in train_dict:\", list(train_dict.keys()))\n",
    "\n",
    "# If Label B exists in train_dict\n",
    "if \"B\" not in train_dict:\n",
    "    print(\"No 'B' in train_dict → nothing to fix for SMOTE. Exiting.\")\n",
    "else:\n",
    "    Xb = train_dict[\"B\"][\"X\"]\n",
    "    yb = train_dict[\"B\"][\"y\"]\n",
    "    print(\"Original B train shape:\", Xb.shape)\n",
    "    print(\"Original B y: total rows =\", len(yb))\n",
    "    print(\"Value counts (including NaN):\")\n",
    "    print(yb.value_counts(dropna=False))\n",
    "    # Ensure numeric\n",
    "    try:\n",
    "        yb_num = pd.to_numeric(yb, errors=\"coerce\")\n",
    "    except Exception as e:\n",
    "        yb_num = yb.copy()\n",
    "    # Drop NaN in y (and corresponding X rows)\n",
    "    mask_valid = yb_num.notna()\n",
    "    n_na = int((~mask_valid).sum())\n",
    "    if n_na > 0:\n",
    "        print(f\"Dropping {n_na} rows from train B where y is NaN.\")\n",
    "        Xb_clean = Xb.loc[mask_valid].reset_index(drop=True)\n",
    "        yb_clean = yb_num.loc[mask_valid].reset_index(drop=True)\n",
    "    else:\n",
    "        Xb_clean = Xb.reset_index(drop=True)\n",
    "        yb_clean = yb_num.reset_index(drop=True)\n",
    "    print(\"After drop: Xb_clean shape:\", Xb_clean.shape)\n",
    "    print(\"After drop: yb_clean value counts:\")\n",
    "    print(yb_clean.value_counts(dropna=False))\n",
    "\n",
    "    unique_classes = sorted(yb_clean.dropna().unique().tolist())\n",
    "    print(\"Unique classes in cleaned y (excluding NaN):\", unique_classes)\n",
    "\n",
    "    if len(unique_classes) < 2:\n",
    "        print(\"\\n==> Cannot apply SMOTE: only one class present after cleaning.\")\n",
    "        print(\"Options now: 1) use class_weight='balanced' during training; 2) collect/label more data for minority class; 3) define another target or relabel.\")\n",
    "        # update train_dict[\"B\"] to cleaned versions (without NaN) and save\n",
    "        train_dict[\"B\"][\"X\"] = Xb_clean\n",
    "        train_dict[\"B\"][\"y\"] = yb_clean\n",
    "        joblib.dump(train_dict, TRAIN_OUT)\n",
    "        print(\"Saved cleaned train_dict (B without NaN) back to\", TRAIN_OUT)\n",
    "    else:\n",
    "        # Try to import SMOTE and run\n",
    "        try:\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "            print(\"imblearn.SMO TE available: applying SMOTE ...\")\n",
    "            Xb_num = Xb_clean.fillna(Xb_clean.median()).values\n",
    "            sm = SMOTE(random_state=42)\n",
    "            X_res, y_res = sm.fit_resample(Xb_num, yb_clean.values)\n",
    "            X_res_df = pd.DataFrame(X_res, columns=Xb_clean.columns)\n",
    "            y_res_ser = pd.Series(y_res)\n",
    "            train_dict[\"B_smote\"] = {\"X\": X_res_df, \"y\": y_res_ser}\n",
    "            # also replace cleaned B (non-smote) so downstream is consistent\n",
    "            train_dict[\"B\"][\"X\"] = Xb_clean\n",
    "            train_dict[\"B\"][\"y\"] = yb_clean\n",
    "            joblib.dump(train_dict, TRAIN_OUT)\n",
    "            # Save CSV copies for convenience\n",
    "            X_res_df.to_csv(DATA_DIR / \"train_X_B_smote.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            y_res_ser.to_csv(DATA_DIR / \"train_y_B_smote.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(\"SMOTE applied: before=%d after=%d\" % (len(Xb_clean), len(X_res_df)))\n",
    "            # update split_info.json\n",
    "            if SPLIT_INFO.exists():\n",
    "                info = json.load(open(SPLIT_INFO, encoding=\"utf-8\"))\n",
    "            else:\n",
    "                info = {}\n",
    "            info.setdefault(\"labelB\", {})\n",
    "            info[\"labelB\"][\"smote_applied\"] = True\n",
    "            info[\"labelB\"][\"n_train_before\"] = int(len(Xb_clean))\n",
    "            info[\"labelB\"][\"n_train_after\"] = int(len(X_res_df))\n",
    "            json.dump(info, open(SPLIT_INFO, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "            print(\"Saved updated split_info.json and train.pkl with B_smote.\")\n",
    "        except Exception as e:\n",
    "            print(\"SMOTE import or application failed with error:\", str(e))\n",
    "            print(\"If you want SMOTE, install imbalanced-learn in this venv: pip install imbalanced-learn\")\n",
    "            # still save cleaned non-smote train\n",
    "            train_dict[\"B\"][\"X\"] = Xb_clean\n",
    "            train_dict[\"B\"][\"y\"] = yb_clean\n",
    "            joblib.dump(train_dict, TRAIN_OUT)\n",
    "            print(\"Saved cleaned train_dict (B without NaN) back to\", TRAIN_OUT)\n",
    "\n",
    "# Also save CSV copies for existing train/test in train_dict/test_dict for convenience\n",
    "for k,v in train_dict.items():\n",
    "    try:\n",
    "        v[\"X\"].to_csv(DATA_DIR / f\"train_X_{k}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        v[\"y\"].to_csv(DATA_DIR / f\"train_y_{k}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save CSV for {k}: {e}\")\n",
    "\n",
    "print(\"Done. Please paste the output here so I can verify. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89573172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE F — Model Training starting...\n",
      "Processing label: A\n",
      "[A] CV logistic: {'accuracy': 0.5672014260249554, 'precision': 0.6029761904761904, 'recall': 0.5607843137254902, 'f1': 0.5780594405594406}\n",
      "[A] CV decision_tree: {'accuracy': 0.5606060606060606, 'precision': 0.6016328939602598, 'recall': 0.5620915032679739, 'f1': 0.5754921189561584}\n",
      "[A] CV random_forest: {'accuracy': 0.5844919786096257, 'precision': 0.6041904761904762, 'recall': 0.673202614379085, 'f1': 0.63172274481944}\n",
      "[A] CV hist_gb: {'accuracy': 0.603030303030303, 'precision': 0.6229509835971002, 'recall': 0.6627450980392158, 'f1': 0.6384565056195352}\n",
      "Saved best model for A -> data\\model_A.pkl\n",
      "Processing label: B\n",
      "Only one class in train for B -> skipping training.\n",
      "PHASE F COMPLETE. Summary saved to data\\modeling_summary.json\n"
     ]
    }
   ],
   "source": [
    "# ===== PHASE F — MODEL TRAINING (1 cell = toàn Phase F) =====\n",
    "# - Requires: data/train.pkl, data/test.pkl (created in Phase E)\n",
    "# - Outputs:\n",
    "#    data/model_labelA.pkl, data/model_labelB.pkl  (best models saved)\n",
    "#    data/model_perf_labelA.json, data/model_perf_labelB.json\n",
    "#    data/modeling_summary.json\n",
    "# - Strategy:\n",
    "#    * For labels with >=2 classes: train LR/DT/RF/GB, CV, evaluate on test, save best by F1\n",
    "#    * For labels with <2 classes: skip training and log warning\n",
    "\n",
    "import joblib, json, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "TRAIN_IN = DATA_DIR / \"train.pkl\"\n",
    "TEST_IN = DATA_DIR / \"test.pkl\"\n",
    "OUT_SUM = DATA_DIR / \"modeling_summary.json\"\n",
    "\n",
    "random_state = 42\n",
    "cv_folds = 5\n",
    "models_to_try = {}\n",
    "\n",
    "print(\"PHASE F — Model Training starting...\")\n",
    "if not TRAIN_IN.exists() or not TEST_IN.exists():\n",
    "    raise FileNotFoundError(\"train.pkl or test.pkl not found in data/. Run Phase E first.\")\n",
    "\n",
    "train_dict = joblib.load(TRAIN_IN)\n",
    "test_dict = joblib.load(TEST_IN)\n",
    "\n",
    "summary = {\"labels\": {}}\n",
    "\n",
    "def evaluate_and_save(label_key, X_train, y_train, X_test, y_test):\n",
    "    result = {\"trained\": False, \"note\": \"\", \"cv\": {}, \"test\": {}}\n",
    "    # check classes\n",
    "    unique = sorted(pd.Series(y_train).dropna().unique().tolist())\n",
    "    result[\"unique_train_classes\"] = unique\n",
    "    if len(unique) < 2:\n",
    "        result[\"note\"] = f\"Only one class in train for {label_key} -> skipping training.\"\n",
    "        print(result[\"note\"])\n",
    "        return result\n",
    "    # ensure y are numeric and no NaN\n",
    "    y_train = pd.to_numeric(pd.Series(y_train).dropna(), errors=\"coerce\")\n",
    "    # convert X to dataframe\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    # standard scaler for LR and GB optional\n",
    "    # Define candidate models\n",
    "    candidates = {}\n",
    "    try:\n",
    "        candidates[\"logistic\"] = Pipeline([\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=random_state))\n",
    "        ])\n",
    "    except Exception:\n",
    "        pass\n",
    "    candidates[\"decision_tree\"] = DecisionTreeClassifier(class_weight=\"balanced\", random_state=random_state)\n",
    "    candidates[\"random_forest\"] = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=random_state, n_jobs=-1)\n",
    "    # use XGBoost if available, else HistGradientBoostingClassifier\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        candidates[\"xgboost\"] = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=random_state)\n",
    "    except Exception:\n",
    "        candidates[\"hist_gb\"] = HistGradientBoostingClassifier(random_state=random_state)\n",
    "\n",
    "    # CV scoring\n",
    "    scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "    for name, model in candidates.items():\n",
    "        try:\n",
    "            cvres = cross_validate(model, X_train, y_train, cv=cv_folds, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "            scores = {metric: float(np.mean(cvres[f\"test_{metric}\"])) for metric in scoring}\n",
    "            result[\"cv\"][name] = scores\n",
    "            print(f\"[{label_key}] CV {name}: {scores}\")\n",
    "        except Exception as e:\n",
    "            result[\"cv\"][name] = {\"error\": str(e)}\n",
    "            print(f\"[{label_key}] CV failed for {name}: {e}\")\n",
    "\n",
    "    # Choose best model by cv f1 (if available) else by accuracy\n",
    "    best_name, best_score = None, -1\n",
    "    for name, vals in result[\"cv\"].items():\n",
    "        if isinstance(vals, dict) and \"f1\" in vals:\n",
    "            if vals[\"f1\"] > best_score:\n",
    "                best_score = vals[\"f1\"]\n",
    "                best_name = name\n",
    "    if best_name is None:\n",
    "        # fallback\n",
    "        for name, vals in result[\"cv\"].items():\n",
    "            if isinstance(vals, dict) and \"accuracy\" in vals:\n",
    "                if vals[\"accuracy\"] > best_score:\n",
    "                    best_score = vals[\"accuracy\"]\n",
    "                    best_name = name\n",
    "\n",
    "    if best_name is None:\n",
    "        result[\"note\"] = \"No candidate model successfully trained in CV.\"\n",
    "        print(result[\"note\"])\n",
    "        return result\n",
    "\n",
    "    # Fit best model on full train and evaluate on test\n",
    "    best_model = candidates[best_name]\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_metrics = {}\n",
    "    try:\n",
    "        test_metrics[\"accuracy\"] = float(accuracy_score(y_test, y_pred))\n",
    "        test_metrics[\"precision\"] = float(precision_score(y_test, y_pred, zero_division=0))\n",
    "        test_metrics[\"recall\"] = float(recall_score(y_test, y_pred, zero_division=0))\n",
    "        test_metrics[\"f1\"] = float(f1_score(y_test, y_pred, zero_division=0))\n",
    "        # roc_auc only if both classes present in test\n",
    "        if len(np.unique(y_test.dropna())) > 1:\n",
    "            prob = best_model.predict_proba(X_test)[:,1] if hasattr(best_model, \"predict_proba\") else None\n",
    "            if prob is not None:\n",
    "                test_metrics[\"roc_auc\"] = float(roc_auc_score(y_test, prob))\n",
    "    except Exception as e:\n",
    "        test_metrics[\"error\"] = str(e)\n",
    "\n",
    "    result[\"trained\"] = True\n",
    "    result[\"best_model\"] = best_name\n",
    "    result[\"test\"] = test_metrics\n",
    "\n",
    "    # Save model object\n",
    "    model_path = DATA_DIR / f\"model_{label_key}.pkl\"\n",
    "    joblib.dump(best_model, model_path)\n",
    "    result[\"model_path\"] = str(model_path)\n",
    "    # Save performance JSON\n",
    "    perf_path = DATA_DIR / f\"model_perf_{label_key}.json\"\n",
    "    json.dump(result, open(perf_path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved best model for {label_key} -> {model_path}\")\n",
    "    return result\n",
    "\n",
    "# iterate labels in train_dict\n",
    "for label_key in sorted(train_dict.keys()):\n",
    "    print(\"Processing label:\", label_key)\n",
    "    entry = {}\n",
    "    X = train_dict[label_key][\"X\"]\n",
    "    y = train_dict[label_key][\"y\"]\n",
    "    # find test split\n",
    "    test_entry = test_dict.get(label_key, None)\n",
    "    if test_entry is None:\n",
    "        print(f\"No test set for {label_key}, skipping evaluation on test set.\")\n",
    "        testX, testy = None, None\n",
    "    else:\n",
    "        testX, testy = test_entry[\"X\"], test_entry[\"y\"]\n",
    "\n",
    "    res = evaluate_and_save(label_key, X, y, testX, testy) if testX is not None else {\"note\":\"no test set\"}\n",
    "    summary[\"labels\"][label_key] = res\n",
    "\n",
    "# Save summary\n",
    "json.dump(summary, open(OUT_SUM, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "print(\"PHASE F COMPLETE. Summary saved to\", OUT_SUM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7fffb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE G — Evaluation starting...\n",
      "Loaded model: data\\model_A.pkl\n",
      "Test shape: (42, 25) y_test length: 42\n",
      "Saved ROC curve to reports\\fig_roc_A.png\n",
      "Saved PR curve to reports\\fig_pr_A.png\n",
      "Saved confusion matrix to reports\\confusion_A.png\n",
      "Saved performance summary to data\\model_perf_summary.json\n",
      "Saved markdown report to reports\\evaluation_report.md\n",
      "\n",
      "PHASE G COMPLETE.\n",
      "Summary metrics: {'accuracy': 0.5476190476190477, 'precision': 0.5517241379310345, 'recall': 0.7272727272727273, 'f1': 0.6274509803921569, 'roc_auc': 0.5045454545454545, 'pr_auc': 0.495120898290996}\n"
     ]
    }
   ],
   "source": [
    "# ===== PHASE G — EVALUATION (1 cell) =====\n",
    "# Outputs:\n",
    "# - reports/evaluation_report.md\n",
    "# - reports/fig_roc_A.png\n",
    "# - reports/fig_pr_A.png\n",
    "# - reports/confusion_A.png\n",
    "# - data/model_perf_summary.json\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, precision_recall_curve, auc, confusion_matrix,\n",
    "                             classification_report)\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DATA_DIR = Path(\"data\")\n",
    "REPORT_DIR = Path(\"reports\")\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_A_PATH = DATA_DIR / \"model_A.pkl\"\n",
    "TEST_X_A = DATA_DIR / \"test_X_A.csv\"\n",
    "TEST_Y_A = DATA_DIR / \"test_y_A.csv\"\n",
    "OUT_SUM = DATA_DIR / \"model_perf_summary.json\"\n",
    "MD_REPORT = REPORT_DIR / \"evaluation_report.md\"\n",
    "FIG_ROC = REPORT_DIR / \"fig_roc_A.png\"\n",
    "FIG_PR = REPORT_DIR / \"fig_pr_A.png\"\n",
    "FIG_CONF = REPORT_DIR / \"confusion_A.png\"\n",
    "\n",
    "ORIG_XLSX = Path(\"/mnt/data/love_survey_responses.xlsx\")  # your uploaded source file (for reference)\n",
    "\n",
    "print(\"PHASE G — Evaluation starting...\")\n",
    "\n",
    "# ----------------- Checks -----------------\n",
    "if not MODEL_A_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Model file not found: {MODEL_A_PATH}\")\n",
    "if not TEST_X_A.exists() or not TEST_Y_A.exists():\n",
    "    raise FileNotFoundError(\"Test X/Y for Label A not found. Please run Phase E to produce test files.\")\n",
    "\n",
    "# ----------------- Load model & test data -----------------\n",
    "model = joblib.load(MODEL_A_PATH)\n",
    "X_test = pd.read_csv(TEST_X_A, encoding=\"utf-8-sig\")\n",
    "y_test = pd.read_csv(TEST_Y_A, encoding=\"utf-8-sig\").iloc[:,0]  # assume single column\n",
    "\n",
    "print(\"Loaded model:\", MODEL_A_PATH)\n",
    "print(\"Test shape:\", X_test.shape, \"y_test length:\", len(y_test))\n",
    "\n",
    "# Ensure numeric types\n",
    "y_test = pd.to_numeric(y_test, errors=\"coerce\")\n",
    "mask_valid = ~y_test.isna()\n",
    "X_test = X_test.loc[mask_valid].reset_index(drop=True)\n",
    "y_test = y_test.loc[mask_valid].reset_index(drop=True)\n",
    "\n",
    "# ----------------- Predictions -----------------\n",
    "y_pred = model.predict(X_test)\n",
    "# Try predict_proba for ROC/AUC\n",
    "probs = None\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    try:\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        probs = None\n",
    "elif hasattr(model, \"decision_function\"):\n",
    "    try:\n",
    "        # decision_function can be used for ranking (not probability)\n",
    "        probs = model.decision_function(X_test)\n",
    "    except Exception:\n",
    "        probs = None\n",
    "\n",
    "# ----------------- Metrics -----------------\n",
    "metrics = {}\n",
    "metrics['accuracy'] = float(accuracy_score(y_test, y_pred))\n",
    "metrics['precision'] = float(precision_score(y_test, y_pred, zero_division=0))\n",
    "metrics['recall'] = float(recall_score(y_test, y_pred, zero_division=0))\n",
    "metrics['f1'] = float(f1_score(y_test, y_pred, zero_division=0))\n",
    "\n",
    "if probs is not None and len(np.unique(y_test)) > 1:\n",
    "    try:\n",
    "        metrics['roc_auc'] = float(roc_auc_score(y_test, probs))\n",
    "    except Exception as e:\n",
    "        metrics['roc_auc'] = None\n",
    "else:\n",
    "    metrics['roc_auc'] = None\n",
    "\n",
    "# PR-AUC\n",
    "if probs is not None:\n",
    "    try:\n",
    "        prec, rec, _ = precision_recall_curve(y_test, probs)\n",
    "        metrics['pr_auc'] = float(auc(rec, prec))\n",
    "    except Exception:\n",
    "        metrics['pr_auc'] = None\n",
    "else:\n",
    "    metrics['pr_auc'] = None\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "clf_report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
    "\n",
    "# ----------------- Save figures -----------------\n",
    "# ROC curve\n",
    "if probs is not None and len(np.unique(y_test)) > 1:\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {metrics.get('roc_auc'):.3f}\" if metrics.get('roc_auc') else \"ROC\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve — Label A\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_ROC)\n",
    "    plt.close()\n",
    "    print(\"Saved ROC curve to\", FIG_ROC)\n",
    "else:\n",
    "    print(\"ROC curve skipped (no probs or single-class test).\")\n",
    "\n",
    "# PR curve\n",
    "if probs is not None:\n",
    "    prec, rec, _ = precision_recall_curve(y_test, probs)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(rec, prec, label=f\"PR AUC = {metrics.get('pr_auc'):.3f}\" if metrics.get('pr_auc') else \"PR\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve — Label A\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_PR)\n",
    "    plt.close()\n",
    "    print(\"Saved PR curve to\", FIG_PR)\n",
    "else:\n",
    "    print(\"PR curve skipped (no probs).\")\n",
    "\n",
    "# Confusion matrix plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix — Label A\")\n",
    "plt.colorbar()\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_CONF)\n",
    "plt.close()\n",
    "print(\"Saved confusion matrix to\", FIG_CONF)\n",
    "\n",
    "# ----------------- Save summary JSON and Markdown report -----------------\n",
    "summary = {\n",
    "    \"label\": \"A\",\n",
    "    \"metrics\": metrics,\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "    \"classification_report\": clf_report,\n",
    "    \"model_path\": str(MODEL_A_PATH),\n",
    "    \"test_shape\": X_test.shape,\n",
    "    \"source_data_file\": str(ORIG_XLSX)\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "with open(OUT_SUM, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved performance summary to\", OUT_SUM)\n",
    "\n",
    "# Save markdown report\n",
    "md_lines = []\n",
    "md_lines.append(\"# Evaluation Report — Label A\\n\")\n",
    "md_lines.append(\"**Model:** `%s`\\n\\n\" % MODEL_A_PATH.name)\n",
    "md_lines.append(\"**Source data file (for reference):** `%s`\\n\\n\" % ORIG_XLSX)\n",
    "md_lines.append(\"## Metrics\\n\")\n",
    "for k,v in metrics.items():\n",
    "    md_lines.append(f\"- **{k}**: {v}\\n\")\n",
    "md_lines.append(\"\\n## Confusion Matrix\\n\")\n",
    "md_lines.append(f\"![confusion]({FIG_CONF.name})\\n\\n\")\n",
    "if (FIG_ROC.exists()):\n",
    "    md_lines.append(\"## ROC Curve\\n\")\n",
    "    md_lines.append(f\"![roc]({FIG_ROC.name})\\n\\n\")\n",
    "if (FIG_PR.exists()):\n",
    "    md_lines.append(\"## Precision-Recall Curve\\n\")\n",
    "    md_lines.append(f\"![pr]({FIG_PR.name})\\n\\n\")\n",
    "md_lines.append(\"## Classification report\\n\")\n",
    "md_lines.append(\"```\\n\")\n",
    "md_lines.append(json.dumps(clf_report, indent=2, ensure_ascii=False))\n",
    "md_lines.append(\"\\n```\\n\")\n",
    "\n",
    "MD = \"\\n\".join(md_lines)\n",
    "with open(MD_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(MD)\n",
    "print(\"Saved markdown report to\", MD_REPORT)\n",
    "\n",
    "print(\"\\nPHASE G COMPLETE.\")\n",
    "print(\"Summary metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4b7024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full features shape: (208, 25)\n",
      "Using test set shape: (42, 25) y length: 42\n",
      "Permutation importance computed and saved to reports/feature_importance.csv\n",
      "Saved perm_importance_bar.png\n",
      "SHAP available: 0.50.0\n",
      "Saved SHAP plots to reports\\shap_plots\n",
      "Done. Check reports/feature_importance.csv and reports/shap_plots/\n"
     ]
    }
   ],
   "source": [
    "# ===== PHASE H FIX (run now that shap is installed) =====\n",
    "import pandas as pd, numpy as np, joblib\n",
    "from pathlib import Path\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "REPORT_DIR = Path(\"reports\")\n",
    "SHAP_DIR = REPORT_DIR / \"shap_plots\"\n",
    "REPORT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "SHAP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "MODEL_A = DATA_DIR / \"model_A.pkl\"\n",
    "X_FULL = DATA_DIR / \"X_features.csv\"\n",
    "TEST_X_A = DATA_DIR / \"test_X_A.csv\"\n",
    "TEST_Y_A = DATA_DIR / \"test_Y_A.csv\"  # note: earlier we used test_Y_A.csv naming; adjust if different\n",
    "# If your test label file is named test_y_A.csv use that instead:\n",
    "if not TEST_Y_A.exists():\n",
    "    alt = DATA_DIR / \"test_y_A.csv\"\n",
    "    if alt.exists():\n",
    "        TEST_Y_A = alt\n",
    "\n",
    "model = joblib.load(MODEL_A)\n",
    "X = pd.read_csv(X_FULL, encoding=\"utf-8-sig\")\n",
    "print(\"Full features shape:\", X.shape)\n",
    "\n",
    "# Use test set to compute permutation importance (ensure alignment)\n",
    "if not TEST_X_A.exists() or not TEST_Y_A.exists():\n",
    "    raise FileNotFoundError(\"Test X/Y for Label A missing. Cannot compute permutation importance on test set.\")\n",
    "X_test = pd.read_csv(TEST_X_A, encoding=\"utf-8-sig\")\n",
    "y_test = pd.read_csv(TEST_Y_A, encoding=\"utf-8-sig\").iloc[:,0]\n",
    "y_test = pd.to_numeric(y_test, errors=\"coerce\")\n",
    "mask = ~y_test.isna()\n",
    "X_test = X_test.loc[mask].reset_index(drop=True)\n",
    "y_test = y_test.loc[mask].reset_index(drop=True)\n",
    "print(\"Using test set shape:\", X_test.shape, \"y length:\", len(y_test))\n",
    "\n",
    "# Permutation importance (on test set)\n",
    "try:\n",
    "    r = permutation_importance(model, X_test, y_test, n_repeats=20, random_state=42, n_jobs=-1)\n",
    "    imp_means = r.importances_mean\n",
    "    fi_df = pd.DataFrame({\"feature\": X_test.columns, \"importance\": imp_means})\n",
    "    fi_df = fi_df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    fi_df.to_csv(REPORT_DIR / \"feature_importance.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Permutation importance computed and saved to reports/feature_importance.csv\")\n",
    "    # plot top 30\n",
    "    topn = min(30, len(fi_df))\n",
    "    plt.figure(figsize=(8,10))\n",
    "    plt.barh(fi_df[\"feature\"].head(topn)[::-1], fi_df[\"importance\"].head(topn)[::-1])\n",
    "    plt.xlabel(\"Permutation importance (mean decrease in score)\")\n",
    "    plt.title(\"Feature importance (permutation on test set)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SHAP_DIR / \"perm_importance_bar.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved perm_importance_bar.png\")\n",
    "except Exception as e:\n",
    "    print(\"Permutation importance failed:\", e)\n",
    "\n",
    "# SHAP (now installed) — compute on up to 100 test samples\n",
    "try:\n",
    "    import shap\n",
    "    print(\"SHAP available:\", shap.__version__)\n",
    "    # use up to 100 rows from X_test\n",
    "    n_sample = min(100, len(X_test))\n",
    "    X_shap = X_test.sample(n=n_sample, random_state=42)\n",
    "    try:\n",
    "        explainer = shap.Explainer(model, X_shap)\n",
    "        sv = explainer(X_shap)\n",
    "        shap.summary_plot(sv, X_shap, show=False)\n",
    "        plt.savefig(SHAP_DIR / \"shap_summary.png\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        shap.plots.bar(sv, show=False)\n",
    "        plt.savefig(SHAP_DIR / \"shap_bar.png\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(\"Saved SHAP plots to\", SHAP_DIR)\n",
    "    except Exception as se:\n",
    "        print(\"SHAP explain failed:\", se)\n",
    "except Exception as e:\n",
    "    print(\"SHAP not available or failed:\", e)\n",
    "\n",
    "print(\"Done. Check reports/feature_importance.csv and reports/shap_plots/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61ef82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE I — Model Delivery starting...\n",
      "Loading model and pipeline...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "STACK_GLOBAL requires str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model and pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     model_A = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pipeline_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     38\u001b[39m     preprocessor = pickle.load(f)\n",
      "\u001b[31mUnpicklingError\u001b[39m: STACK_GLOBAL requires str"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ===== PHASE I — MODEL DELIVERY (FULL) =====\n",
    "# ============================================\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "print(\"PHASE I — Model Delivery starting...\")\n",
    "\n",
    "BASE = Path(\".\")\n",
    "DATA = BASE / \"data\"\n",
    "MODELS = BASE / \"models\"\n",
    "REPORTS = BASE / \"reports\"\n",
    "\n",
    "# Ensure models/ exists\n",
    "MODELS.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Load trained model + preprocessing pipeline\n",
    "# -------------------------------------------------------\n",
    "model_path = DATA / \"model_A.pkl\"\n",
    "pipeline_path = DATA / \"feature_pipeline.pkl\"\n",
    "perf_path = DATA / \"model_perf_summary.json\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    raise FileNotFoundError(\"model_A.pkl not found — ensure Phase F finished correctly.\")\n",
    "\n",
    "if not pipeline_path.exists():\n",
    "    raise FileNotFoundError(\"feature_pipeline.pkl not found — ensure Phase D finished correctly.\")\n",
    "\n",
    "print(\"Loading model and pipeline...\")\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model_A = pickle.load(f)\n",
    "\n",
    "with open(pipeline_path, \"rb\") as f:\n",
    "    preprocessor = pickle.load(f)\n",
    "\n",
    "performance = {}\n",
    "if perf_path.exists():\n",
    "    with open(perf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        performance = json.load(f)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Combine into a single deliverable bundle\n",
    "# -------------------------------------------------------\n",
    "bundle = {\n",
    "    \"model_name\": \"LoveSurvey_Model_A\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"algorithm\": str(type(model_A)),\n",
    "    \"preprocessor\": preprocessor,\n",
    "    \"model\": model_A,\n",
    "    \"performance\": performance,\n",
    "}\n",
    "\n",
    "bundle_path = MODELS / \"model_bundle.pkl\"\n",
    "with open(bundle_path, \"wb\") as f:\n",
    "    pickle.dump(bundle, f)\n",
    "\n",
    "print(f\"Saved unified model bundle -> {bundle_path}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Export info file for deployment\n",
    "# -------------------------------------------------------\n",
    "deploy_info = {\n",
    "    \"model_bundle\": str(bundle_path),\n",
    "    \"input_features_expected\": list(preprocessor.get_feature_names_out()),\n",
    "    \"raw_data_source\": \"D:/UIT/Semester 5/Vocational Skills/love_survey_responses.xlsx\",\n",
    "    \"generated_features\": list(preprocessor.get_feature_names_out()),\n",
    "    \"metrics\": performance,\n",
    "    \"note\": \"This package contains preprocessor + model for Label A prediction.\",\n",
    "}\n",
    "\n",
    "deploy_info_path = MODELS / \"deployment_info.json\"\n",
    "with open(deploy_info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(deploy_info, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved deployment info -> {deploy_info_path}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Create prediction template file (predict.py)\n",
    "# -------------------------------------------------------\n",
    "predict_script = BASE / \"predict_example.py\"\n",
    "predict_script.write_text(\n",
    "    \"\"\"\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# === Load model bundle ===\n",
    "with open(\"models/model_bundle.pkl\", \"rb\") as f:\n",
    "    bundle = pickle.load(f)\n",
    "\n",
    "model = bundle[\"model\"]\n",
    "pre = bundle[\"preprocessor\"]\n",
    "\n",
    "# === Predict function ===\n",
    "def predict(df: pd.DataFrame):\n",
    "    X = pre.transform(df)\n",
    "    prob = model.predict_proba(X)[:, 1]\n",
    "    pred = model.predict(X)\n",
    "    return pred, prob\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: load 1 row from CSV\n",
    "    df = pd.read_csv(\"example_input.csv\")\n",
    "    pred, prob = predict(df)\n",
    "    print(\"Prediction:\", pred)\n",
    "    print(\"Probability:\", prob)\n",
    "    \"\"\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"Saved example prediction script -> predict_example.py\")\n",
    "\n",
    "print(\"\\nPHASE I COMPLETE — Model delivery bundle created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d53379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trying to load: data\\model_bundle.pkl  (size: MISSING bytes) ---\n",
      "  -> File not found.\n",
      "\n",
      "--- Trying to load: data\\model_A.pkl  (size: 91360 bytes) ---\n",
      "  -> joblib.load SUCCESS. Type: <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>\n",
      "\n",
      "--- Trying to load: data\\feature_pipeline.pkl  (size: 32564 bytes) ---\n",
      "  -> joblib.load SUCCESS. Type: <class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "\n",
      "=== Summary of attempts ===\n",
      "data\\model_bundle.pkl -> NOT LOADED\n",
      "data\\model_A.pkl -> loaded by joblib type: <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>\n",
      "   sklearn estimator class: <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>\n",
      "   repr: HistGradientBoostingClassifier(random_state=42)\n",
      "data\\feature_pipeline.pkl -> loaded by joblib type: <class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "   sklearn estimator class: <class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "   repr: ColumnTransformer(sparse_threshold=0,\n",
      "                  transformers=[('num',\n",
      "                                 Pipeline(steps=[('impute',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scale', StandardScaler())]\n",
      "\n",
      "If none loaded successfully, likely reasons:\n",
      "- model/pipeline were serialized with cloudpickle or contain lambda/local classes.\n",
      "- custom transformers were defined only in notebook and not importable now.\n",
      "- different python/pickle versions.\n",
      "\n",
      "Recommended next steps (if not loaded):\n",
      "1) If you created custom Transformer classes in this notebook, run the cells that define them BEFORE loading bundle.\n",
      "2) Try re-saving objects using joblib.dump or cloudpickle.dump from the same environment that created them and re-load here.\n",
      "3) If you want, paste the output above here and I will advise the exact fix.\n"
     ]
    }
   ],
   "source": [
    "# ===== Try multiple safe ways to load model/pipeline files =====\n",
    "import os, joblib, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "files_to_try = [\n",
    "    Path(\"data/model_bundle.pkl\"),\n",
    "    Path(\"data/model_A.pkl\"),\n",
    "    Path(\"data/feature_pipeline.pkl\")\n",
    "]\n",
    "\n",
    "def try_load(path):\n",
    "    print(\"\\n--- Trying to load:\", path, \" (size:\", path.stat().st_size if path.exists() else 'MISSING', \"bytes) ---\")\n",
    "    if not path.exists():\n",
    "        print(\"  -> File not found.\")\n",
    "        return None\n",
    "    # 1) try joblib.load (common for sklearn)\n",
    "    try:\n",
    "        obj = joblib.load(path)\n",
    "        print(\"  -> joblib.load SUCCESS. Type:\", type(obj))\n",
    "        return (\"joblib\", obj)\n",
    "    except Exception as e:\n",
    "        print(\"  -> joblib.load FAILED:\", repr(e))\n",
    "    # 2) try pickle.load (default)\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"  -> pickle.load SUCCESS. Type:\", type(obj))\n",
    "        return (\"pickle\", obj)\n",
    "    except Exception as e:\n",
    "        print(\"  -> pickle.load FAILED:\", repr(e))\n",
    "    # 3) try pickle with encoding latin1 (for numpy arrays pickled in py2->py3)\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            obj = pickle.load(f, encoding=\"latin1\")\n",
    "        print(\"  -> pickle.load(encoding='latin1') SUCCESS. Type:\", type(obj))\n",
    "        return (\"pickle-latin1\", obj)\n",
    "    except Exception as e:\n",
    "        print(\"  -> pickle.load(encoding='latin1') FAILED:\", repr(e))\n",
    "    # 4) try cloudpickle (if installed)\n",
    "    try:\n",
    "        import cloudpickle\n",
    "        with open(path, \"rb\") as f:\n",
    "            obj = cloudpickle.load(f)\n",
    "        print(\"  -> cloudpickle.load SUCCESS. Type:\", type(obj))\n",
    "        return (\"cloudpickle\", obj)\n",
    "    except Exception as e:\n",
    "        print(\"  -> cloudpickle.load FAILED or cloudpickle not installed:\", repr(e))\n",
    "    # 5) fallback: read header bytes to inspect\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            hdr = f.read(200)\n",
    "        print(\"  -> File header (first 200 bytes):\", hdr[:200])\n",
    "    except Exception as e:\n",
    "        print(\"  -> Cannot read header:\", repr(e))\n",
    "    return None\n",
    "\n",
    "results = {}\n",
    "for p in files_to_try:\n",
    "    results[str(p)] = try_load(p)\n",
    "\n",
    "print(\"\\n=== Summary of attempts ===\")\n",
    "for k,v in results.items():\n",
    "    if v is None:\n",
    "        print(k, \"-> NOT LOADED\")\n",
    "    else:\n",
    "        mtype, obj = v\n",
    "        print(k, \"-> loaded by\", mtype, \"type:\", type(obj))\n",
    "        # If it's a dict, show keys\n",
    "        if isinstance(obj, dict):\n",
    "            print(\"   dict keys:\", list(obj.keys()))\n",
    "        # If it's a sklearn estimator, show class\n",
    "        try:\n",
    "            from sklearn.base import BaseEstimator\n",
    "            if isinstance(obj, BaseEstimator):\n",
    "                print(\"   sklearn estimator class:\", obj.__class__)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # show repr short\n",
    "        try:\n",
    "            s = repr(obj)\n",
    "            print(\"   repr:\", s[:300])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"\\nIf none loaded successfully, likely reasons:\")\n",
    "print(\"- model/pipeline were serialized with cloudpickle or contain lambda/local classes.\")\n",
    "print(\"- custom transformers were defined only in notebook and not importable now.\")\n",
    "print(\"- different python/pickle versions.\")\n",
    "print(\"\\nRecommended next steps (if not loaded):\")\n",
    "print(\"1) If you created custom Transformer classes in this notebook, run the cells that define them BEFORE loading bundle.\")\n",
    "print(\"2) Try re-saving objects using joblib.dump or cloudpickle.dump from the same environment that created them and re-load here.\")\n",
    "print(\"3) If you want, paste the output above here and I will advise the exact fix.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5b39c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE I REBUILD: starting...\n",
      "Loading model and pipeline with joblib...\n",
      "Loaded model type: <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>\n",
      "Loaded preprocessor type: <class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "Could not get_feature_names_out() from preprocessor: Estimator freq does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?\n",
      "Fallback: loaded feature names from data/X_features.csv. Count: 25\n",
      "Saved model bundle via joblib -> models\\model_bundle.pkl\n",
      "Saved deployment info -> models\\deployment_info.json\n",
      "Saved predict example -> predict_example.py\n",
      "\n",
      "PHASE I REBUILD complete. Files created:\n",
      " - models\\model_bundle.pkl\n",
      " - models\\deployment_info.json\n",
      " - predict_example.py\n"
     ]
    }
   ],
   "source": [
    "# === CELL: Rebuild model bundle safely using joblib (run this in a NEW cell) ===\n",
    "import joblib, json, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path(\".\")\n",
    "DATA = BASE / \"data\"\n",
    "MODELS = BASE / \"models\"\n",
    "MODELS.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths\n",
    "model_path = DATA / \"model_A.pkl\"\n",
    "pipeline_path = DATA / \"feature_pipeline.pkl\"\n",
    "perf_path = DATA / \"model_perf_summary.json\"\n",
    "x_features_path = DATA / \"X_features.csv\"  # fallback for feature names\n",
    "\n",
    "print(\"PHASE I REBUILD: starting...\")\n",
    "\n",
    "# 1) load with joblib (we already know joblib.load works)\n",
    "print(\"Loading model and pipeline with joblib...\")\n",
    "model_A = joblib.load(model_path)\n",
    "preprocessor = joblib.load(pipeline_path)\n",
    "print(\"Loaded model type:\", type(model_A))\n",
    "print(\"Loaded preprocessor type:\", type(preprocessor))\n",
    "\n",
    "# 2) load performance if exists\n",
    "performance = {}\n",
    "if perf_path.exists():\n",
    "    with open(perf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            performance = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Warning: cannot read perf file:\", e)\n",
    "\n",
    "# 3) try to get feature names expected by preprocessor\n",
    "input_features = None\n",
    "try:\n",
    "    # ColumnTransformer (sklearn >=1.0) supports get_feature_names_out()\n",
    "    input_features = list(preprocessor.get_feature_names_out())\n",
    "    print(\"Extracted feature names from preprocessor (get_feature_names_out). Count:\", len(input_features))\n",
    "except Exception as e:\n",
    "    print(\"Could not get_feature_names_out() from preprocessor:\", e)\n",
    "    # fallback: try to read X_features.csv\n",
    "    if x_features_path.exists():\n",
    "        try:\n",
    "            Xf = pd.read_csv(x_features_path, encoding=\"utf-8-sig\")\n",
    "            input_features = list(Xf.columns)\n",
    "            print(\"Fallback: loaded feature names from data/X_features.csv. Count:\", len(input_features))\n",
    "        except Exception as e2:\n",
    "            print(\"Fallback failed to read X_features.csv:\", e2)\n",
    "            input_features = []\n",
    "\n",
    "# 4) build bundle dict and save using joblib.dump\n",
    "bundle = {\n",
    "    \"model_name\": \"LoveSurvey_Model_A\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"model\": model_A,\n",
    "    \"preprocessor\": preprocessor,\n",
    "    \"performance\": performance\n",
    "}\n",
    "bundle_path = MODELS / \"model_bundle.pkl\"\n",
    "joblib.dump(bundle, bundle_path)\n",
    "print(\"Saved model bundle via joblib ->\", bundle_path)\n",
    "\n",
    "# 5) write deployment_info.json\n",
    "deploy_info = {\n",
    "    \"model_bundle\": str(bundle_path),\n",
    "    \"model_file\": str(model_path),\n",
    "    \"preprocessor_file\": str(pipeline_path),\n",
    "    \"input_features_expected\": input_features,\n",
    "    \"raw_data_source_example\": r\"D:\\\\UIT\\\\Semester 5\\\\Vocational Skills\\\\love_survey_responses.xlsx\",\n",
    "    \"notes\": \"Bundle saved with joblib.dump for robust loading. Use joblib.load to load bundle.\"\n",
    "}\n",
    "deploy_path = MODELS / \"deployment_info.json\"\n",
    "with open(deploy_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(deploy_info, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved deployment info ->\", deploy_path)\n",
    "\n",
    "# 6) write predict_example.py (overwrite)\n",
    "predict_path = BASE / \"predict_example.py\"\n",
    "predict_code = f'''import joblib\n",
    "import pandas as pd\n",
    "\n",
    "bundle = joblib.load(\"models/model_bundle.pkl\")\n",
    "model = bundle[\"model\"]\n",
    "pre = bundle[\"preprocessor\"]\n",
    "\n",
    "def predict(df: pd.DataFrame):\n",
    "    X_trans = pre.transform(df)\n",
    "    pred = model.predict(X_trans)\n",
    "    prob = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            prob = model.predict_proba(X_trans)[:, 1]\n",
    "        except Exception:\n",
    "            prob = None\n",
    "    return pred, prob\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: provide a CSV with columns matching input_features\n",
    "    df = pd.read_csv(\"example_input.csv\")\n",
    "    p, prob = predict(df)\n",
    "    print(\"pred:\", p)\n",
    "    print(\"prob:\", prob)\n",
    "'''\n",
    "predict_path.write_text(predict_code, encoding=\"utf-8\")\n",
    "print(\"Saved predict example ->\", predict_path)\n",
    "\n",
    "print(\"\\nPHASE I REBUILD complete. Files created:\")\n",
    "print(\" -\", bundle_path)\n",
    "print(\" -\", deploy_path)\n",
    "print(\" -\", predict_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ba871b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved example_input.csv (1 row) — sửa nếu cần rồi chạy predict_example.py\n",
      "Columns: ['bạn_đánh_giá_sức_khoẻ_thể_chất_của_mình_như_thế_nào', 'bạn_cảm_thấy_ngoại_hình_của_mình_như_thế_nào', 'bạn_có_cảm_thấy_áp_lực_từ_gia_đình_hoặc_xã_hội_về_việc_phải_có_người_yêu_không', 'tình_yêu_ảnh_hưởng_đến_kết_quả_học_tập_sự_nghiệp_ở_mức_độ_như_thế_nào', 'bạn_cảm_thấy_gia_đình_có_ảnh_hưởng_đến_mối_quan_hệ_của_bạn_như_thế_nào', 'xu_hướng_tính_dục_của_bạn_là_Dị tính', 'bạn_có_mong_muốn_tìm_được_người_yêu_trong_tương_lai_gần_không_0', 'bạn_có_mong_muốn_tìm_được_người_yêu_trong_tương_lai_gần_không_Chưa chắc', 'có_nên_tìm_kiếm_người_yêu_khi_đang_xây_dựng_sự_nghiệp_Không nên', 'có_nên_tìm_kiếm_người_yêu_khi_đang_xây_dựng_sự_nghiệp_Nên', 'nên_chi_tiêu_bao_nhiêu_hàng_tháng_cho_việc_hẹn_hò_500.000 đồng - 1.000.000 đồng', 'nên_chi_tiêu_bao_nhiêu_hàng_tháng_cho_việc_hẹn_hò_Trên 1.000.000 đồng', 'bạn_nghĩ_tình_yêu_có_cần_gắn_với_tình_dục_không_0', 'bạn_nghĩ_tình_yêu_có_cần_gắn_với_tình_dục_không_1', 'bạn_nghĩ_ở_tuổi_nào_nên_kết_hôn_là_hợp_lý_25 – 27', 'bạn_nghĩ_ở_tuổi_nào_nên_kết_hôn_là_hợp_lý_28 – 30', 'bạn_nghĩ_ở_tuổi_nào_nên_kết_hôn_là_hợp_lý_Trên 30', 'nếu_chưa_thì_có_những_lý_do_nào_khiến_bạn_chưa_có_người_yêu', 'tiêu_chí_quan_trọng_nhất_với_bạn_khi_chọn_người_yêu', 'bạn_đã_từng_thử_tiếp_cận_tình_yêu_qua_cách_nào', 'nếu_có_thì_bạn_đã_chia_tay_vì_những_lý_do_gì', 'nên_ưu_tiên_yếu_tố_gì_khi_chọn_địa_điểm_hẹn_hò_cùng_người_yêu', 'theo_bạn_đâu_là_lý_do_khiến_nhiều_sinh_viên_chia_tay_nhất', 'yếu_tố_nào_gây_mất_điểm_nhất_khi_hẹn_hò', 'những_lỗi_lầm_nào_có_thể_tha_thứ_trong_tình_yêu']\n"
     ]
    }
   ],
   "source": [
    "# Tạo example_input.csv từ X_features.csv (1 row mẫu)\n",
    "import pandas as pd\n",
    "X = pd.read_csv(\"data/X_features.csv\", encoding=\"utf-8-sig\")\n",
    "X.head(1).to_csv(\"example_input.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved example_input.csv (1 row) — sửa nếu cần rồi chạy predict_example.py\")\n",
    "print(\"Columns:\", list(X.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
